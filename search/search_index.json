{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introducing Orb Orb is a modern network observability platform built to provide critical visibility into increasingly complex, segmented, and distributed networks. It can discover devices, analyze network traffic, run synthetic network probes, and connect telemetry to your existing observability stacks with OpenTelemetry. Orb differentiates from other solutions by providing agent fleet management and allowing for dynamic reconfiguration of remote agents in real time. Understand Understand traffic patterns, identify malicious activity and anomalies, hunt down misconfigurations, and improve network performance across your ever-changing infrastructure by deeply analyzing packets, DNS, flow, connectivity, latency, and more. Smartly Analyze and Collect Shorten time-to-action and lower costs by dynamically applying composable policies designed to filter, aggregate and extract the exact information you need, without having to ship raw data to central data warehouses or expensive SaaS services Orchestrate Agents are controlled in real time from the Orb Portal or automated through the REST API, allowing instantaneous policy updates across even massive fleets. Compose data pipelines from targeted groups of agents with precise instructions, sending the results to modern observability stacks via OpenTelemetry Integrate The Orb control plane is a modern microservices application with well defined APIs that can be deployed to any Kubernetes service in a private or public cloud, avoiding vendor lock-in. The Orb agent is lightweight and modular, and can scale down to embedded platforms or up to full servers. Orb is designed to easily integrate into the larger Network Automation ecosystem. Where Orb fits Orb \u2019s focus is enabling network automation and visibility across network device control and data planes using a dynamically reconfigurable agent based system. It is free, open-source software released under Mozilla Public License (MPL).","title":"Home"},{"location":"#introducing-orb","text":"Orb is a modern network observability platform built to provide critical visibility into increasingly complex, segmented, and distributed networks. It can discover devices, analyze network traffic, run synthetic network probes, and connect telemetry to your existing observability stacks with OpenTelemetry. Orb differentiates from other solutions by providing agent fleet management and allowing for dynamic reconfiguration of remote agents in real time.","title":"Introducing Orb"},{"location":"#where-orb-fits","text":"Orb \u2019s focus is enabling network automation and visibility across network device control and data planes using a dynamically reconfigurable agent based system. It is free, open-source software released under Mozilla Public License (MPL).","title":"Where Orb fits"},{"location":"about/","text":"Orb is a new kind of observability platform that makes it easier for operators and developers to gain a deeper understanding of their networks, distributed applications, and traffic flows in real time. Orb integrates with your observability stack, providing dynamic orchestration of observability agents that extract business intelligence at the edge. The platform is completely open source , extensible, vendor neutral, and cloud native. The components The agents Orb manages observability agents that collect network data from applications, systems, and edge locations (VMs, containers, servers) in real time. An agent acts as a sensor installed next to a data source so it can collect, analyze, and summarize information. You run agents on your edge locations and orchestrate them via the control plane. While ingesting a high volume of information-dense data streams, the agents translate this information to deliver consumable, actionable datasets. Developers and network operators can view this data locally at the edge via the agent's command-line interface (CLI) and globally in a central database via a standard dashboard tool, such as Grafana. The control plane Orb combines concepts from edge computing, the Internet of Things (IoT), and high-throughput stream processing. As an IoT-inspired cloud control plane , Orb connects a fleet of distributed observability agents (such as the open source pktvisor ) deployed at the edge and gives you command over that fleet. In operating the control plane, you issue instructions to the agents, dynamically programming and re-programming them with data-collection policies to build different datasets in real time. The features Orb orchestrates network observability policies across a fleet of agents on the edge\u2014providing you with lightweight, immediately actionable results. Plugs into popular observability stacks, like Prometheus and Elasticsearch, as well as cloud storage and data pipelines Built using a cloud-native, microservices-based architecture Offers a self-hosted (via Docker Compose or Kubernetes) or a SaaS option Orb focuses on edge analysis, preferring \u201csmall data\u201d-style, actionable metrics over the collection and storage of terabytes of raw, inscrutable data. Allows you to visualize and automate on data at the edge for a hyper-real-time local view or centrally in the cloud for a global view Streamlines data collection and exporting back to the control plane where it is available for analytics, security, automation, etc. Provides a single pane of glass across all sensors Orb + pktvisor Via Orb's user interface , you decide what data to extract from which agents . The resource-efficient, side-car style pktvisor observability agent performs edge analysis on network data streams. This combination allows you to: Adjust analysis and collection parameters dynamically across the entire fleet via a powerful control plane Perform centralized fleet management, allowing you to configure heartbeats, tagging, and grouping for each of the pktvisor agents Orchestrate dataset policies that specify the type of data to extract from each agent In terms of metrics, pktvisor can capture DNS, DHCP, and L2/L3 network data via packet capture, dnstap , sflow , among other input methods. For a complete list of metrics currently collected by pktvisor, look here . To view a Grafana dashboard for visualizing pktvisor Prometheus metrics, look here . Core Concepts The concepts below comprise Orb\u2019s architecture. Agent This is a sensor installed next to a data source at the edge so it can summarize, analyze, and collect information. Agent group This is a list of simple key/value pairs that match against agent tags to dynamically define a group of agents. For example, \u201cregion: US\u201d will group all agents in the fleet that have this key/value set in their tags. Fleet This is a collection of agents that may be widely distributed and number in the tens, hundreds, or thousands\u2014all of which are able to connect and contribute to the same observability system. Policies These are instructions sent to the agents to define how to collect metrics . It is backend-specific information needed at the edge. Dataset These are instructions that describe how specific agents in the fleet (matched according to a given agent group) should apply collection policies and where they should sink their data . Orb will manage many datasets concurrently. Sinks This is where you send the data. This is the system that collects the data and allows you to sync it to different locations . Currently, Orb supports Prometheus but will support more backends in the future.","title":"About"},{"location":"about/#the-components","text":"","title":"The components"},{"location":"about/#the-agents","text":"Orb manages observability agents that collect network data from applications, systems, and edge locations (VMs, containers, servers) in real time. An agent acts as a sensor installed next to a data source so it can collect, analyze, and summarize information. You run agents on your edge locations and orchestrate them via the control plane. While ingesting a high volume of information-dense data streams, the agents translate this information to deliver consumable, actionable datasets. Developers and network operators can view this data locally at the edge via the agent's command-line interface (CLI) and globally in a central database via a standard dashboard tool, such as Grafana.","title":"The agents"},{"location":"about/#the-control-plane","text":"Orb combines concepts from edge computing, the Internet of Things (IoT), and high-throughput stream processing. As an IoT-inspired cloud control plane , Orb connects a fleet of distributed observability agents (such as the open source pktvisor ) deployed at the edge and gives you command over that fleet. In operating the control plane, you issue instructions to the agents, dynamically programming and re-programming them with data-collection policies to build different datasets in real time.","title":"The control plane"},{"location":"about/#the-features","text":"Orb orchestrates network observability policies across a fleet of agents on the edge\u2014providing you with lightweight, immediately actionable results. Plugs into popular observability stacks, like Prometheus and Elasticsearch, as well as cloud storage and data pipelines Built using a cloud-native, microservices-based architecture Offers a self-hosted (via Docker Compose or Kubernetes) or a SaaS option Orb focuses on edge analysis, preferring \u201csmall data\u201d-style, actionable metrics over the collection and storage of terabytes of raw, inscrutable data. Allows you to visualize and automate on data at the edge for a hyper-real-time local view or centrally in the cloud for a global view Streamlines data collection and exporting back to the control plane where it is available for analytics, security, automation, etc. Provides a single pane of glass across all sensors","title":"The features"},{"location":"about/#orb-pktvisor","text":"Via Orb's user interface , you decide what data to extract from which agents . The resource-efficient, side-car style pktvisor observability agent performs edge analysis on network data streams. This combination allows you to: Adjust analysis and collection parameters dynamically across the entire fleet via a powerful control plane Perform centralized fleet management, allowing you to configure heartbeats, tagging, and grouping for each of the pktvisor agents Orchestrate dataset policies that specify the type of data to extract from each agent In terms of metrics, pktvisor can capture DNS, DHCP, and L2/L3 network data via packet capture, dnstap , sflow , among other input methods. For a complete list of metrics currently collected by pktvisor, look here . To view a Grafana dashboard for visualizing pktvisor Prometheus metrics, look here .","title":"Orb + pktvisor"},{"location":"about/#core-concepts","text":"The concepts below comprise Orb\u2019s architecture.","title":"Core Concepts"},{"location":"about/#agent","text":"This is a sensor installed next to a data source at the edge so it can summarize, analyze, and collect information.","title":"Agent"},{"location":"about/#agent-group","text":"This is a list of simple key/value pairs that match against agent tags to dynamically define a group of agents. For example, \u201cregion: US\u201d will group all agents in the fleet that have this key/value set in their tags.","title":"Agent group"},{"location":"about/#fleet","text":"This is a collection of agents that may be widely distributed and number in the tens, hundreds, or thousands\u2014all of which are able to connect and contribute to the same observability system.","title":"Fleet"},{"location":"about/#policies","text":"These are instructions sent to the agents to define how to collect metrics . It is backend-specific information needed at the edge.","title":"Policies"},{"location":"about/#dataset","text":"These are instructions that describe how specific agents in the fleet (matched according to a given agent group) should apply collection policies and where they should sink their data . Orb will manage many datasets concurrently.","title":"Dataset"},{"location":"about/#sinks","text":"This is where you send the data. This is the system that collects the data and allows you to sync it to different locations . Currently, Orb supports Prometheus but will support more backends in the future.","title":"Sinks"},{"location":"contact/","text":"Community Contribute Orb is an open source project! Work with us on GitHub and star the project to show your interest. Star Contact We are very interested to hear about your use cases, feature requests, and contribution ideas. File an issue Join us on Slack in the #orb channel Check out the NetBox Labs YouTube channel Send mail to info@netboxlabs.com Policies Our policies here at NetBox Labs Terms of Service Privacy Policy Explore Articles Orb Data Sheet: Actionable Edge Observability Using DNS to Minimize Cyber Threat Exposure Deep Network Traffic Observability with Pktvisor and Prometheus Q4 2021 Update on NS1 Labs: pktvisor, Orb, NetBox Cloud Extracting the Signal: Rethinking Network Observability Orb - A New Paradigm for Dynamic Edge Observability A Wave of Open Source Innovation at NS1 Labs with Orb and NetBox NS1 Launches Innovation Lab to Solve Challenges in Modern Application Delivery and Edge Networking Conference Presentations CHI-NOG 11 , 2023 - Recording NANOG 85 Montreal: On the Edge of Small Data , 2022 - Recording , Slides PromCon North America: Deep Network Traffic Observability , 2021 - Recording , Slides NS1 INSIGHTS 2021: Build the Better Future , 2021 - Recording ICANN DNS Symposium , 2021 - Recording , Slides DNS-OARC , 2020 - Recording (first talk), Slides O'Reilly Velocity San Jose , 2019 - Recording , Slides","title":"Community"},{"location":"contact/#community","text":"","title":"Community"},{"location":"contact/#contribute","text":"Orb is an open source project! Work with us on GitHub and star the project to show your interest. Star","title":"Contribute"},{"location":"contact/#contact","text":"We are very interested to hear about your use cases, feature requests, and contribution ideas. File an issue Join us on Slack in the #orb channel Check out the NetBox Labs YouTube channel Send mail to info@netboxlabs.com","title":"Contact"},{"location":"contact/#policies","text":"Our policies here at NetBox Labs Terms of Service Privacy Policy","title":"Policies"},{"location":"contact/#explore","text":"","title":"Explore"},{"location":"contact/#articles","text":"Orb Data Sheet: Actionable Edge Observability Using DNS to Minimize Cyber Threat Exposure Deep Network Traffic Observability with Pktvisor and Prometheus Q4 2021 Update on NS1 Labs: pktvisor, Orb, NetBox Cloud Extracting the Signal: Rethinking Network Observability Orb - A New Paradigm for Dynamic Edge Observability A Wave of Open Source Innovation at NS1 Labs with Orb and NetBox NS1 Launches Innovation Lab to Solve Challenges in Modern Application Delivery and Edge Networking","title":"Articles"},{"location":"contact/#conference-presentations","text":"CHI-NOG 11 , 2023 - Recording NANOG 85 Montreal: On the Edge of Small Data , 2022 - Recording , Slides PromCon North America: Deep Network Traffic Observability , 2021 - Recording , Slides NS1 INSIGHTS 2021: Build the Better Future , 2021 - Recording ICANN DNS Symposium , 2021 - Recording , Slides DNS-OARC , 2020 - Recording (first talk), Slides O'Reilly Velocity San Jose , 2019 - Recording , Slides","title":"Conference Presentations"},{"location":"getting_started/","text":"Info You can find detailed installation instructions in the Documentation section of this site ( Installation ). Connect to Orb Register a new account After registering , you should see the home page with a welcome message. Deploy your first Agent Follow the steps below after logging in to the Orb Portal to get an Orb agent up and running. Create Agent Credentials You create a set of agent credentials for each node you want to monitor. Agents are organized by tags. Each agent has a set of corresponding credentials used during provisioning. You may also provision agents directly at the edge instead of through the UI. Navigate to Agents , and then click New Agent . Fill in an Agent Name and click Next . Optionally, fill in Key and Value tags, then click the + on the right side of the menu. These tags represent the way you will assign the agent to an agent group. Reasonable tags might be \"location\", \"region\", \"pop\", \"type\", etc. You should see an icon with your key and value tags appear above the Key and Value textboxes. Click Next . Click Save to confirm your agent\u2019s name and tags. Your agent credentials should appear. Copy the Provisioning Command . This command contains all the information you need to run the Docker container with the given credentials you now have for the agent. Start your Agent Warning To connect an agent to the self-hosted development control plane , disable TLS verification using the enviromental variable below in your provisioning command: -e ORB_TLS_VERIFY = false Check an example here . Run the Provisioning Command into a terminal on the node where you want your agent to run. See Running Orb Agent for more details. Close out of the Agent Credentials menu. Refresh the Agents List in UI. The agent you just created should display an Online status. Optionally, click the agent's name to view the Agent View screen. This screen will contain more information as you add the agent to an agent group and add corresponding policies and datasets. Configure your Agent Create an Agent Group Agents are organized into agent groups based on key-value tag matching. Navigate to Agent Groups , and then click New Agent Group . Fill in an Agent Group Name and click Next . Fill in the Key and Value tags, which need to match the tags of the corresponding Agent , and click the + on the right side of the menu. You should see an icon with your key and value tags appear above the Key and Value textboxes. Click Next . You should see a message about the number of agents matching. Then click Save . By clicking in EXPAND you can see the agents that are matching with the group (This is optional). View the newly created group in the Agent Groups list. View Matching Agents Click the number in the Agents column to view the matching agents. Create a Sink A sink is a location to send the metrics collected from the agents. The current version supports OpenTelemetry Protocol (OTLP) and Prometheus. Try Orb with OTLP \ud83d\udd25 Prometheus You can use a free Grafana Cloud account as a sink. Navigate to Sink Management , and then click New Sink . Fill in a sink name and set sink backend to Otlphtp. Optionally, add a description and sink tags by filling in the Tag Key and Tag Value fields. Click + after each key-value pair. Fill in your sink destination details and click Create . View your newly created sink in the All Sinks list. You can use a private Prometheus instance or use a free Grafana Cloud account as a sink. Navigate to Sink Management , and then click New Sink . Fill in a sink name and set sink backend to Prometheus. Optionally, add a description and sink tags by filling in the Tag Key and Tag Value fields. Click + after each key-value pair. Fill in your sink destination details. This includes the host/username/password from your Prometheus remote_write configuration. Click Create . View your newly created sink in the All Sinks list. Create a Policy Policies tell agents which metrics to collect and how to collect them. Navigate to Policy Management , and then click New Policy . Fill in a policy name and (optionally) a description and tags. The policy name needs to be unique and cannot contain spaces (use underscores or dashes instead). Then click Next . You can start using the suggested policy. Click Next . In this policy, tap (input stream) is \u201cdefault_pcap\u201d which is the default for Packet Capture. Handlers specifies how to analyze the input stream selected and, in this case, we want to analyze DNS and Network (L2-L3) traffic. For a more tailored observability policy to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference . Click Save to save the policy. Create a Dataset Datasets essentially connect all the previous pieces. By creating and defining a dataset, you send a specific policy to a specific agent group and establish a sink to receive the resulting metrics which allows you to visualize and action on the data. Navigate to the DATASETS tab in Policy View Navigate to DATASETS tab in the policy you would like to create a Dataset for, then click on New Dataset. Select the Agent Group and Sink(s) and click Create The policy will be sent in real time to the Agents in the Agent Group and begin running. Verify policy running Navigate to Agents and click on the name of the agent that matches the group you selected in creating the dataset. The Agent View screen displays under the Policies & Datasets tab information about your policy and dataset. Your policy must have a running status. Check Orb Health Orb objects have variables status whose functions are to help you understand the health of your system. A guide on how to interpret this is provided below. Agent Status There are 4 expected status for agents: new , online , offline and stale These status are related to the agent's last activity (heartbeat): \ud83d\udfe3 new means that the agent never sent a heartbeat (i.e. has never connected to the control plane) \ud83d\udfe2 online means that the agent is sending heartbeats right now (is running and healthy). \u26aa offline means that the control plane received a heartbeat saying that the agent is going offline. \ud83d\udfe0 stale means that the control plane has not received a heartbeat for 5 minutes (without having received a heartbeat stating that it would go offline) Policies Status The status of each policy can be seen on the preview page of an agent to which it is applied The policy will be: running if agent policy is being managed from the control plane (policy-related metrics are being requested/scraped by this agent) failed_to_apply if an error prevents the policy from being applied by the agent. By clicking on the expand icon you can see the cause of the error offline if the policy was stopped by agent request Datasets Validity Once created a dataset can only be valid (\ud83d\udfe2) or invalid (\ud83d\udd34) The dataset will always be valid as long as the policy, the group AND the sink linked to it exist in Orb. If the policy, the group OR the sink is removed from Orb, the dataset will become invalid . Note, in the image above, that the invalid dataset does not contain the group listed, as it has been removed from the Orb. Sinks Status \ud83d\udfe3 Unknown - No metrics have ever been published to this sink \ud83d\udfe2 Active - Metrics are actively being published to this sink \ud83d\udfe0 Idle - The last metrics published to this sink were more than 5 minutes ago \ud83d\udd34 Error - The sink tried to publish the metrics but failed. Attention : In this case, check that the sink credentials are configured correctly. \ud83d\udd35 Provisioning - Intermediate status which means that the sink collector is being provisioned. Visualize and alert on your metrics Your agent should now be running the policy you created. After one minute of collection time, the metrics will be sent to your Prometheus sink. You may use standard tools for visualizing and alerting on your Prometheus metrics. A popular option is Grafana . A pre-made dashboard for visualizing Orb/pktvisor metrics is available for import here .","title":"Getting Started"},{"location":"getting_started/#connect-to-orb","text":"Register a new account After registering , you should see the home page with a welcome message.","title":"Connect to Orb"},{"location":"getting_started/#deploy-your-first-agent","text":"Follow the steps below after logging in to the Orb Portal to get an Orb agent up and running.","title":"Deploy your first Agent"},{"location":"getting_started/#create-agent-credentials","text":"You create a set of agent credentials for each node you want to monitor. Agents are organized by tags. Each agent has a set of corresponding credentials used during provisioning. You may also provision agents directly at the edge instead of through the UI. Navigate to Agents , and then click New Agent . Fill in an Agent Name and click Next . Optionally, fill in Key and Value tags, then click the + on the right side of the menu. These tags represent the way you will assign the agent to an agent group. Reasonable tags might be \"location\", \"region\", \"pop\", \"type\", etc. You should see an icon with your key and value tags appear above the Key and Value textboxes. Click Next . Click Save to confirm your agent\u2019s name and tags. Your agent credentials should appear. Copy the Provisioning Command . This command contains all the information you need to run the Docker container with the given credentials you now have for the agent.","title":"Create Agent Credentials"},{"location":"getting_started/#start-your-agent","text":"Warning To connect an agent to the self-hosted development control plane , disable TLS verification using the enviromental variable below in your provisioning command: -e ORB_TLS_VERIFY = false Check an example here . Run the Provisioning Command into a terminal on the node where you want your agent to run. See Running Orb Agent for more details. Close out of the Agent Credentials menu. Refresh the Agents List in UI. The agent you just created should display an Online status. Optionally, click the agent's name to view the Agent View screen. This screen will contain more information as you add the agent to an agent group and add corresponding policies and datasets.","title":"Start your Agent"},{"location":"getting_started/#configure-your-agent","text":"","title":"Configure your Agent"},{"location":"getting_started/#create-an-agent-group","text":"Agents are organized into agent groups based on key-value tag matching. Navigate to Agent Groups , and then click New Agent Group . Fill in an Agent Group Name and click Next . Fill in the Key and Value tags, which need to match the tags of the corresponding Agent , and click the + on the right side of the menu. You should see an icon with your key and value tags appear above the Key and Value textboxes. Click Next . You should see a message about the number of agents matching. Then click Save . By clicking in EXPAND you can see the agents that are matching with the group (This is optional). View the newly created group in the Agent Groups list. View Matching Agents Click the number in the Agents column to view the matching agents.","title":"Create an Agent Group"},{"location":"getting_started/#create-a-sink","text":"A sink is a location to send the metrics collected from the agents. The current version supports OpenTelemetry Protocol (OTLP) and Prometheus. Try Orb with OTLP \ud83d\udd25 Prometheus You can use a free Grafana Cloud account as a sink. Navigate to Sink Management , and then click New Sink . Fill in a sink name and set sink backend to Otlphtp. Optionally, add a description and sink tags by filling in the Tag Key and Tag Value fields. Click + after each key-value pair. Fill in your sink destination details and click Create . View your newly created sink in the All Sinks list. You can use a private Prometheus instance or use a free Grafana Cloud account as a sink. Navigate to Sink Management , and then click New Sink . Fill in a sink name and set sink backend to Prometheus. Optionally, add a description and sink tags by filling in the Tag Key and Tag Value fields. Click + after each key-value pair. Fill in your sink destination details. This includes the host/username/password from your Prometheus remote_write configuration. Click Create . View your newly created sink in the All Sinks list.","title":"Create a Sink"},{"location":"getting_started/#create-a-policy","text":"Policies tell agents which metrics to collect and how to collect them. Navigate to Policy Management , and then click New Policy . Fill in a policy name and (optionally) a description and tags. The policy name needs to be unique and cannot contain spaces (use underscores or dashes instead). Then click Next . You can start using the suggested policy. Click Next . In this policy, tap (input stream) is \u201cdefault_pcap\u201d which is the default for Packet Capture. Handlers specifies how to analyze the input stream selected and, in this case, we want to analyze DNS and Network (L2-L3) traffic. For a more tailored observability policy to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference . Click Save to save the policy.","title":"Create a Policy"},{"location":"getting_started/#create-a-dataset","text":"Datasets essentially connect all the previous pieces. By creating and defining a dataset, you send a specific policy to a specific agent group and establish a sink to receive the resulting metrics which allows you to visualize and action on the data. Navigate to the DATASETS tab in Policy View Navigate to DATASETS tab in the policy you would like to create a Dataset for, then click on New Dataset. Select the Agent Group and Sink(s) and click Create The policy will be sent in real time to the Agents in the Agent Group and begin running.","title":"Create a Dataset"},{"location":"getting_started/#verify-policy-running","text":"Navigate to Agents and click on the name of the agent that matches the group you selected in creating the dataset. The Agent View screen displays under the Policies & Datasets tab information about your policy and dataset. Your policy must have a running status.","title":"Verify policy running"},{"location":"getting_started/#check-orb-health","text":"Orb objects have variables status whose functions are to help you understand the health of your system. A guide on how to interpret this is provided below.","title":"Check Orb Health"},{"location":"getting_started/#agent-status","text":"There are 4 expected status for agents: new , online , offline and stale These status are related to the agent's last activity (heartbeat): \ud83d\udfe3 new means that the agent never sent a heartbeat (i.e. has never connected to the control plane) \ud83d\udfe2 online means that the agent is sending heartbeats right now (is running and healthy). \u26aa offline means that the control plane received a heartbeat saying that the agent is going offline. \ud83d\udfe0 stale means that the control plane has not received a heartbeat for 5 minutes (without having received a heartbeat stating that it would go offline)","title":"Agent Status"},{"location":"getting_started/#policies-status","text":"The status of each policy can be seen on the preview page of an agent to which it is applied The policy will be: running if agent policy is being managed from the control plane (policy-related metrics are being requested/scraped by this agent) failed_to_apply if an error prevents the policy from being applied by the agent. By clicking on the expand icon you can see the cause of the error offline if the policy was stopped by agent request","title":"Policies Status"},{"location":"getting_started/#datasets-validity","text":"Once created a dataset can only be valid (\ud83d\udfe2) or invalid (\ud83d\udd34) The dataset will always be valid as long as the policy, the group AND the sink linked to it exist in Orb. If the policy, the group OR the sink is removed from Orb, the dataset will become invalid . Note, in the image above, that the invalid dataset does not contain the group listed, as it has been removed from the Orb.","title":"Datasets Validity"},{"location":"getting_started/#sinks-status","text":"\ud83d\udfe3 Unknown - No metrics have ever been published to this sink \ud83d\udfe2 Active - Metrics are actively being published to this sink \ud83d\udfe0 Idle - The last metrics published to this sink were more than 5 minutes ago \ud83d\udd34 Error - The sink tried to publish the metrics but failed. Attention : In this case, check that the sink credentials are configured correctly. \ud83d\udd35 Provisioning - Intermediate status which means that the sink collector is being provisioned.","title":"Sinks Status"},{"location":"getting_started/#visualize-and-alert-on-your-metrics","text":"Your agent should now be running the policy you created. After one minute of collection time, the metrics will be sent to your Prometheus sink. You may use standard tools for visualizing and alerting on your Prometheus metrics. A popular option is Grafana . A pre-made dashboard for visualizing Orb/pktvisor metrics is available for import here .","title":"Visualize and alert on your metrics"},{"location":"saas/","text":"The Orb software-as-a-service platform ( orb.live ) is now in development. Check back soon for the official release of the SaaS platform. Until then, interact with us today and check out pktvisor , a production-ready observability agent.","title":"Saas"},{"location":"api/orb_rest_api/","text":"Orb REST API Docs Follow the links below for API documentation of each respective Orb microservice: Fleet Policies Sinks","title":"Orb REST API"},{"location":"api/orb_rest_api/#orb-rest-api-docs","text":"Follow the links below for API documentation of each respective Orb microservice: Fleet Policies Sinks","title":"Orb REST API Docs"},{"location":"documentation/advanced_policies/","text":"Orb Policy Reference The policy model has seven top level sections: name , desciption , tags , backend , schema_version , policy and format . JSON { \"name\" : \"\" , \"description\" : \"\" , \"tags\" : {}, \"backend\" : \"\" , \"schema_version\" : \"\" , \"format\" : \"\" , \"policy\" : {} } Policy Sections Default Required name - \u2705 backend - \u2705 policy - \u2705 description None \u274c tags None \u274c schema_version 1.0 \u274c format json \u274c Name Policy name must be unique, must start with a letter and contain only letters, numbers, \"-\" or \"_\" . Examples Success \"name\": \"my_policy\" \"name\": \"MY-policy_2 Failure \"name\": \"1_policy\" \"name\": \"MY-policy/2 Description Policy description is a free string. You can describe using spaces, letters, numbers and special characters. Tags Tags are intended to facilitate organization and are a dict type. The tags usage syntax is: JSON \"tags\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" , \"key3\" : \"value3\" } Backend Backend determine to which backend the policy will be attached. Check the available backends here . Schema version Each backend supported on Orb must have a policy schema to be validated, parsed and applied. schema_version is the field responsible for allowing different schema versions and backward compatibility. Default should be \"1.0\". Format The format specifies in which format the policy data will be written. The options are json and yaml . json is the default value. Policy Each supported backend has specific structures for its policy data. Check the available options below. Pktvisor policy The policy data for pktvisor can be written in either YAML or JSON, and has four top level sections: \u201cinput\u201d, \u201chandlers\u201d, \"config\" and \u201ckind\u201d. YAML JSON input : .. config : ... kind : collection handlers : ... { \"input\" : { }, \"config\" : { }, \"kind\" : \"collection\" }, \"handlers\" : { } Input section The input section specifies what data streams the policy will be using for analysis, in other words, this specifies what data the agent should be listening in on, and is defined at the agent level . Required fields: input_type - the type of input. This field will be validated with the type of tap indicated by the tap parameter or by the tap selector . If the types are incompatible, the policy will fail. tap - the name given to this input in the tap/agent configuration or tap_selector - tags to match existing agent taps . If tap_selector is used, it can be chosen whether taps with any of the tags or with all tags will be attached. Optional fields: filter - to specify what data to include from the input config - how the input will be used Note Every configuration set at the input can be reset at the tap level, with the one set on the tap dominant over the one set on the input. Default input structure Using specific tap ( check the application in a policy here ): YAML JSON input : tap : tap_name input_type : type_of_input filter : bpf : ... config : ... { \"input\" : { \"tap\" : \"tap_name\" , \"input_type\" : \"type_of_input\" , \"filter\" : { \"bpf\" : \"...\" }, \"config\" : \"...\" } } or using tap selector matching any ( check the application in a policy here ): YAML JSON input : tap_selector : any : - key1 : value1 - key2 : value2 input_type : type_of_input filter : bpf : ... config : ... { \"input\" : { \"tap_selector\" : { \"any\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ] }, \"input_type\" : \"type_of_input\" , \"filter\" : { \"bpf\" : \"...\" }, \"config\" : \"...\" } } or using tap selector matching all ( check the application in a policy here ): YAML JSON input : tap_selector : all : - key1 : value1 - key2 : value2 input_type : type_of_input filter : bpf : ... config : ... { \"input\" : { \"tap_selector\" : { \"all\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ] }, \"input_type\" : \"type_of_input\" , \"filter\" : { \"bpf\" : \"...\" }, \"config\" : \"...\" } } Config section There is the possibility of defining settings on the policy level. Currently, the only configuration available is the merge_like_handlers . Policy Configuration Type Default merge_like_handlers bool false merge_like_handlers When merge_like_handlers config is true, metrics from all handlers of the same type are scraped together. This is useful when the tap_selector is used, as, by default, metrics are generated separately for each tap in the policy and this can be very expensive, depending on the number of taps. The merge_like_handlers filter usage syntax is: YAML JSON config : merge_like_handlers : true { \"config\" : { \"merge_like_handlers\" : true } } Kind section What kind of object you want to create The only option for now is \"collection\" . Handlers section (Analysis) Handlers are the modules responsible for extracting metrics from inputs. For each handler type, specific configuration, filters and group of metrics can be defined, and there are also configs (abstract configuration) that can be applied to all handlers: Default handler structure: YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 5 topn_count : 10 topn_percentile_threshold : 0 modules : tap_name : type : ... require_version : ... config : ... filter : ... metric_groups : enable : - ... - .... disable : - ..... - ...... { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 5 , \"topn_count\" : 10 , \"topn_percentile_threshold\" : 0 }, \"modules\" : { \"tap_name\" : { \"type\" : \"...\" , \"require_version\" : \"...\" , \"config\" : \"...\" , \"filter\" : \"...\" , \"metric_groups\" : { \"enable\" : [ \"...\" , \"....\" ], \"disable\" : [ \".....\" , \"......\" ] } } } } } To enable any metric group use the syntax: YAML JSON metric_groups : enable : - group_to_enable { \"metric_groups\" : { \"enable\" : [ \"group_to_enable\" ] } } To enable all available metric groups use the syntax: YAML JSON metric_groups : enable : - all { \"metric_groups\" : { \"enable\" : [ \"all\" ] } } In order to disable any metric group use the syntax: YAML JSON metric_groups : disable : - group_to_disable { \"metric_groups\" : { \"disable\" : [ \"group_to_disable\" ] } } To disable all metric groups use the syntax: YAML JSON metric_groups : disable : - all { \"metric_groups\" : { \"disable\" : [ \"all\" ] } } Attention: enable is dominant over disable. So if both are passed, the metrics group will be enabled; Abstract Configurations There are general configurations, which can be applied to all handlers. These settings can be reset for each module, within the specific module configs. In this case, the configuration inside the module will override the configuration passed in general handler. Abstract Configuration Type Default deep_sample_rate int 100 (per second) num_periods int 5 topn_count int 10 topn_percentile_threshold int 0 deep_sample_rate Back to Top - Abstract Configurations deep_sample_rate determines the number of data packets that will be analyzed deeply per second. Some metrics are operationally expensive to generate, such as metrics that require string parsing (qname2, qtype, etc.). For this reason, a maximum number of packets per second to be analyzed is determined. If in one second fewer packages than the maximum amount are transacted, all packages will compose the deep metrics sample, if there are more packages than the established one, the value of the variable will be used. Allowed values are in the range [1,100]. Default value is 100. Note If a value less than 1 is passed, the deep_sample_rate will be 1. If the value passed is more than 100, deep_sample_rate will be 100. The deep_sample_rate usage syntax is: YAML JSON deep_sample_rate : int { \"deep_sample_rate\" : i nt } num_periods Back to Top - Abstract Configurations num_periods determines the amount of minutes of data that will be available on the metrics endpoint. Allowed values are in the range [2,10]. Default value is 5. The num_periods usage syntax is: YAML JSON num_periods : int { \"num_periods\" : i nt } topn_count Back to Top - Abstract Configurations topn_count sets the maximum amount of elements displayed in top metrics. If there is less quantity than the configured value, the composite metrics will have the existing value. But if there are more metrics than the configured value, the variable will be actively limiting. Any positive integer is valid and the default value is 10. The topn_count usage syntax is: YAML JSON topn_count : int { \"topn_count\" : i nt } topn_percentile_threshold Back to Top - Abstract Configurations topn_percentile_threshold sets the threshold of data to be considered based on the percentiles, so allowed values are in the range [0,100]. The default value is 0, that is, all data is considered. If, for example, the value 10 is set, scraped topn metrics will only consider data from the 10th percentile, that is, data between the highest 90%. The topn_percentile_threshold usage syntax is: YAML JSON topn_percentile_threshold : int { \"topn_percentile_threshold\" : i nt } DNS Analyzer (dns) Example of policy Metrics Group Filters Configurations DNS(v2) DNS(v1) Warning Status: Beta . The metric names and configuration options may still change Example of policy with input pcap and handler DNS(v2) YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 5 topn_count : 10 modules : default_dns : type : dns require_version : \"2.0\" config : public_suffix_list : true deep_sample_rate : 50 num_periods : 2 topn_count : 25 topn_percentile_threshold : 10 filter : only_rcode : 0 only_dnssec_response : true answer_count : 1 only_qtype : [ 1 , 2 ] only_qname_suffix : [ \".google.com\" , \".orb.live\" ] geoloc_notfound : false asn_notfound : false dnstap_msg_type : \"auth\" metric_groups : enable : - cardinality - counters - quantiles - top_rcodes - top_qnames - top_qtypes disable : - top_size - top_ports - xact_times - top_ecs input : input_type : pcap tap : default_pcap filter : bpf : udp port 53 config : iface : wlo1 host_spec : 192.168.1.167/24 pcap_source : libpcap debug : true config : merge_like_handlers : true kind : collection { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 5 , \"topn_count\" : 10 }, \"modules\" : { \"default_dns\" : { \"type\" : \"dns\" , \"require_version\" : \"2.0\" , \"config\" : { \"public_suffix_list\" : true , \"deep_sample_rate\" : 50 , \"num_periods\" : 2 , \"topn_count\" : 25 , \"topn_percentile_threshold\" : 10 }, \"filter\" : { \"only_rcode\" : 0 , \"only_dnssec_response\" : true , \"answer_count\" : 1 , \"only_qtype\" : [ 1 , 2 ], \"only_qname_suffix\" : [ \".google.com\" , \".orb.live\" ], \"geoloc_notfound\" : false , \"asn_notfound\" : false , \"dnstap_msg_type\" : \"auth\" }, \"metric_groups\" : { \"enable\" : [ \"cardinality\" , \"counters\" , \"quantiles\" , \"top_rcodes\" , \"top_qnames\" , \"top_qtypes\" ], \"disable\" : [ \"top_size\" , \"top_ports\" , \"xact_times\" , \"top_ecs\" ] } } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"default_pcap\" , \"filter\" : { \"bpf\" : \"udp port 53\" }, \"config\" : { \"iface\" : \"wlo1\" , \"host_spec\" : \"192.168.1.167/24\" , \"pcap_source\" : \"libpcap\" , \"debug\" : true } }, \"config\" : { \"merge_like_handlers\" : true }, \"kind\" : \"collection\" } Handler Type : \"dns\" Metrics Group - Check the dns metrics belonging to each group Metric Group Default top_ecs disabled top_ports disabled top_size disabled xact_times disabled cardinality enabled counters enabled top_qnames enabled quantiles enabled top_qtypes enabled top_rcodes enabled Filters Filter Type Input only_rcode str[] PCAP exclude_noerror bool PCAP only_dnssec_response bool PCAP answer_count int PCAP only_qtype str[] PCAP only_qname str[] PCAP only_qname_suffix str[] PCAP geoloc_notfound bool PCAP asn_notfound bool PCAP only_xact_directions str[] PCAP dnstap_msg_type str DNSTAP only_rcode: str[] . Back to DNS-v2 filters list Input: PCAP When a DNS server returns a response to a query made, one of the properties of the response is the \"response code\" (rcode), a code that describes what happened to the query that was made. Most response codes indicate why the query failed and when the query succeeds, the return is an RCODE:0, whose name is NOERROR. Supported types are in the table below (if you use any other code that is not in the table below, your policy will fail): DNS response code Name Description Reference 0 NOERROR No error condition [RFC1035] 1 FORMERR Format error - The name server was unable to interpret the query. [RFC1035] 2 SERVFAIL Server failure - The name server was unable to process this query due to a problem with the name server. [RFC1035] 3 NXDOMAIN Name Error - Meaningful only for responses from an authoritative name server, this code signifies that the domain name referenced in the query does not exist. [RFC1035] 4 NOTIMP Not Implemented - The name server does not support the requested kind of query. [RFC1035] 5 REFUSED The name server refuses to perform the specified operation for policy reasons. For example, a name server may not wish to provide the information to the particular requester, or a name server may not wish to perform a particular operation (e.g., zone) [RFC1035] 6 YXDOMAIN Name that should not exist, does exist [RFC2136] 7 YXRRSET RR set that should not exist, does exist [RFC2136] 8 NXRRSET RR Set that should exist, does not exist [RFC2136] 9 NOTAUTH Server Not Authoritative for zone or Not Authorized [RFC2136] 10 NOTZONE Name not contained in zone [RFC2136] 11 DSOTYPENI DSO-TYPE Not Implemented [RFC8490] 16 BADVERS/BADSIG Bad OPT Version or TSIG Signature Failure [RFC8945] 17 BADKEY Key not recognized [RFC8945] 18 BADTIME Signature out of time window [RFC8945] 19 BADMODE Bad TKEY Mode [RFC2930] 20 BADNAME Duplicate key name [RFC2930] 21 BADALG Algorithm not supported [RFC2930] 22 BADTRUNC Bad Truncation [RFC8945] 23 BADCOOKIE Bad/missing Server Cookie [RFC7873] The only_rcode filter usage syntax is: YAML JSON only_rcode : - str - str { \"only_rcode\" : [ \"str\" , \"int\" ] } with the int referring to the response code to be filtered, written as string. Example: If you want to filter only successful queries responses you should use (note that all that the query will be discarded and the result will be just the responses): YAML JSON only_rcode : - \"NXDOMAIN\" - \"2\" { \"only_rcode\" : [ \"NXDOMAIN\" , \"2\" ] } Important information is that only one response code is possible for each handler. So, in order to have multiple filters on the same policy, multiple handlers must be created, each with a rcode type. exclude_noerror: bool Back to DNS-v2 filters list Input: PCAP You may still want to filter out only responses with any kind of error. For this, there is the exclude_noerror filter, which removes from its results all responses that did not return any type of error. The exclude_noerror filter usage syntax is: YAML JSON exclude_noerror : true { \"exclude_noerror\" : true } Attention: the filter of exclude_noerror is dominant in relation to the filter of only_rcode, that is, if the filter of exclude_noerror is true, even if the filter of only_rcode is set, the results will be composed only by responses without any type of error (all type of errors will be kept). only_dnssec_response: bool Back to DNS-v2 filters list Input: PCAP When you make a DNS query, the response you get may have a DNSSEC signature, which authenticates that DNS records originate from an authorized sender, thus protecting DNS from falsified information. To filter only responses signed by an authorized sender, use: The only_dnssec_response filter usage syntax is: YAML JSON only_dnssec_response : true { \"only_dnssec_response\" : true } answer_count: int Back to DNS-v2 filters list Input: PCAP One of the properties present in the query message structure is Answer RRs , which is the count of entries in the responses section (RR stands for \u201cresource record\u201d). The number of answers in the query is always zero, as a query message has only questions and no answers, and when the server sends the answer to that query, the value is set to the amount of entries in the answers section. The answer_count filter usage syntax is: YAML JSON answer_count : int { \"answer_count\" : i nt } with the int referring to the desired amount of answer. Note that any value greater than zero that is defined will exclude queries from the results, since in queries the number of answers is always 0. As the answers count of queries is 0, whenever the value set for the answer_count is 0, both queries and responses will compose the result. A special case is the concept of NODATA , which is one of the possible returns to a query made to a DNS server is known as. This happens when the query is successful (so rcode:0), but there is no data as a response, so the number of answers is 0. In this case, to have in the results only the cases of NODATA , that is, the responses, the filter must be used together with the filter exclude_noerror . Important information is that only one answer_count is possible for each handler. So, in order to have multiple counts on the same policy, multiple handlers must be created, each with an amount of answers. only_qtype: str[] Back to DNS-v2 filters list Input: PCAP DNS record types are records that provide important information about a hostname or domain. Supported default types can be seen here . The only_qtype filter usage syntax is: YAML JSON only_qtype : - str - str { \"only_qtype\" : [ \"str\" , \"str\" ] } If you want to filter only IPV4 record types, for example, you should use: YAML JSON only_qtype : - \"A\" { \"only_qtype\" : [ \"A\" ] } or YAML JSON only_qtype : - 1 { \"only_qtype\" : [ 1 ] } Multiple types are also supported and both queries and responses that have any of the values in the array will be considered. YAML JSON only_qtype : - 1 - 2 - \"A\" { \"only_qtype\" : [ 1 , 2 , \"A\" ] } only_qname: str[] Back to DNS-v2 filters list Input: PCAP The only_qname filters dns packets based on queries and responses whose names exactly matches the strings present in the array. The only_qname filter usage syntax is: YAML JSON only_qname : - str - str { \"only_qname\" : [ \"str\" , \"str\" ] } Examples: YAML JSON only_qname : - www.google.com - .nsone.net { \"only_qname\" : [ \"www.google.com\" , \".nsone.net\" ] } only_qname_suffix: str[] Back to DNS-v2 filters list Input: PCAP The only_qname_suffix filters queries and responses whose endings (suffixes) of the names match the strings present in the array. The only_qname_suffix filter usage syntax is: YAML JSON only_qname_suffix : - str - str { \"only_qname_suffix\" : [ \"str\" , \"str\" ] } Examples: YAML JSON only_qname_suffix : - .google.com { \"only_qname_suffix\" : [ \".google.com\" ] } or YAML JSON only_qname_suffix : - google.com - .nsone.net { \"only_qname_suffix\" : [ \"google.com\" , \".nsone.net\" ] } geoloc_notfound: bool Back to DNS-v2 filters list Input: PCAP Based on ECS (EDNS Client Subnet) information, it is possible to determine the geolocation of where the query is being made. When the Subnet refers to a region found in the standard databases, the city, state and country (approximated) are returned. However, when based on the subnet it is not possible to determine the geolocation, a not found is returned. The geoloc_notfound filter only keeps responses whose geolocations were not found. The geoloc_notfound filter usage syntax is: YAML JSON geoloc_notfound : true { \"geoloc_notfound\" : true } asn_notfound: bool Back to DNS-v2 filters list Input: PCAP Based on ECS (EDNS Client Subnet) information, it is possible to determine the ASN (Autonomous System Number). When the IP of the subnet belongs to some not known ASN in the standard databases, a not found is returned. The asn_notfound filter only keeps responses whose asn were not found. The asn_notfound filter usage syntax is: YAML JSON asn_notfound : true { \"asn_notfound\" : true } only_xact_directions: str[] Back to DNS-v2 filters list Input: PCAP Filters metrics according to the direction of the transaction. Options are: in , out and unknown . YAML JSON only_xact_directions : - str - str Example: only_xact_directions : - in - unknown { \"only_xact_directions\" : [ \"str\" , \"str\" ] } Example: { \"only_xact_directions\" : [ \"in\" , \"unknown\" ] } dnstap_msg_type: str Back to DNS-v2 filters list Input: DNSTAP With a dnstap protocol it is possible to know the type of message that must be resolved in the request to the server. This filter therefore allows you to filter by response types. Supported message types are: auth , resolver , client , forwarder , stub , tool and update . The dnstap_msg_type filter usage syntax is: YAML JSON dnstap_msg_type : str Example: dnstap_msg_type : auth { \"dnstap_msg_type\" : \"str\" } Example: { \"dnstap_msg_type\" : \"auth\" } Example of policy with input pcap and handler DNS(v1) YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 5 topn_count : 10 modules : default_dns : type : dns config : public_suffix_list : true deep_sample_rate : 50 num_periods : 2 topn_count : 25 topn_percentile_threshold : 10 filter : only_rcode : 0 only_dnssec_response : true answer_count : 1 only_qtype : [ 1 , 2 ] only_qname_suffix : [ \".google.com\" , \".orb.live\" ] geoloc_notfound : false asn_notfound : false dnstap_msg_type : \"auth\" metric_groups : enable : - top_ecs - top_qnames_details disable : - histograms - quantiles - cardinality - counters - dns_transaction - top_qnames - top_ports input : input_type : pcap tap : default_pcap filter : bpf : udp port 53 config : iface : wlo1 host_spec : 192.168.1.167/24 pcap_source : libpcap debug : true config : merge_like_handlers : true kind : collection { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 5 , \"topn_count\" : 10 }, \"modules\" : { \"default_dns\" : { \"type\" : \"dns\" , \"config\" : { \"public_suffix_list\" : true , \"deep_sample_rate\" : 50 , \"num_periods\" : 2 , \"topn_count\" : 25 , \"topn_percentile_threshold\" : 10 }, \"filter\" : { \"only_rcode\" : 0 , \"only_dnssec_response\" : true , \"answer_count\" : 1 , \"only_qtype\" : [ 1 , 2 ], \"only_qname_suffix\" : [ \".google.com\" , \".orb.live\" ], \"geoloc_notfound\" : false , \"asn_notfound\" : false , \"dnstap_msg_type\" : \"auth\" }, \"metric_groups\" : { \"enable\" : [ \"top_ecs\" , \"top_qnames_details\" ], \"disable\" : [ \"cardinality\" , \"counters\" , \"dns_transaction\" , \"top_qnames\" , \"top_ports\" ] } } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"default_pcap\" , \"filter\" : { \"bpf\" : \"udp port 53\" }, \"config\" : { \"iface\" : \"wlo1\" , \"host_spec\" : \"192.168.1.167/24\" , \"pcap_source\" : \"libpcap\" , \"debug\" : true } }, \"config\" : { \"merge_like_handlers\" : true }, \"kind\" : \"collection\" } Handler Type : \"dns\" Metrics Group - Check the dns metrics belonging to each group Metric Group Default top_ecs disabled top_qnames_details disabled histograms disabled cardinality enabled counters enabled dns_transaction enabled top_qnames enabled top_ports enabled quantiles enabled Filters Filter Type Input only_rcode str[] PCAP exclude_noerror bool PCAP only_dnssec_response bool PCAP answer_count int PCAP only_qtype str[] PCAP only_qname str[] PCAP only_qname_suffix str[] PCAP geoloc_notfound bool PCAP asn_notfound bool PCAP only_queries bool PCAP only_responses bool PCAP dnstap_msg_type str DNSTAP only_rcode: str[] . Back to DNS-v1 filters list Input: PCAP When a DNS server returns a response to a query made, one of the properties of the response is the \"response code\" (rcode), a code that describes what happened to the query that was made. Most response codes indicate why the query failed and when the query succeeds, the return is an RCODE:0, whose name is NOERROR. Supported types are in the table below (if you use any other code that is not in the table below, your policy will fail): DNS response code Name Description Reference 0 NOERROR No error condition [RFC1035] 1 FORMERR Format error - The name server was unable to interpret the query. [RFC1035] 2 SERVFAIL Server failure - The name server was unable to process this query due to a problem with the name server. [RFC1035] 3 NXDOMAIN Name Error - Meaningful only for responses from an authoritative name server, this code signifies that the domain name referenced in the query does not exist. [RFC1035] 4 NOTIMP Not Implemented - The name server does not support the requested kind of query. [RFC1035] 5 REFUSED The name server refuses to perform the specified operation for policy reasons. For example, a name server may not wish to provide the information to the particular requester, or a name server may not wish to perform a particular operation (e.g., zone) [RFC1035] 6 YXDOMAIN Name that should not exist, does exist [RFC2136] 7 YXRRSET RR set that should not exist, does exist [RFC2136] 8 NXRRSET RR Set that should exist, does not exist [RFC2136] 9 NOTAUTH Server Not Authoritative for zone or Not Authorized [RFC2136] 10 NOTZONE Name not contained in zone [RFC2136] 11 DSOTYPENI DSO-TYPE Not Implemented [RFC8490] 16 BADVERS/BADSIG Bad OPT Version or TSIG Signature Failure [RFC8945] 17 BADKEY Key not recognized [RFC8945] 18 BADTIME Signature out of time window [RFC8945] 19 BADMODE Bad TKEY Mode [RFC2930] 20 BADNAME Duplicate key name [RFC2930] 21 BADALG Algorithm not supported [RFC2930] 22 BADTRUNC Bad Truncation [RFC8945] 23 BADCOOKIE Bad/missing Server Cookie [RFC7873] The only_rcode filter usage syntax is: YAML JSON only_rcode : - str - str { \"only_rcode\" : [ \"str\" , \"int\" ] } with the int referring to the response code to be filtered, written as string. Example: If you want to filter only successful queries responses you should use (note that all that the query will be discarded and the result will be just the responses): YAML JSON only_rcode : - \"NXDOMAIN\" - \"2\" { \"only_rcode\" : [ \"NXDOMAIN\" , \"2\" ] } Important information is that only one response code is possible for each handler. So, in order to have multiple filters on the same policy, multiple handlers must be created, each with a rcode type. exclude_noerror: bool Back to DNS-v1 filters list Input: PCAP The exclude_noerror filter removes from its results all responses that did not return any type of error (RCODE=0) The exclude_noerror filter usage syntax is: YAML JSON exclude_noerror : true { \"exclude_noerror\" : true } exclude_noerror takes precedence over the \"only_rcode\" filter. If exclude_noerror = True, then the results will be composed only of responses WITH an error, that is, responses with RCODE=0 will not be returned. **only_dnssec_response:** *bool* <a name=\"only_dnssec_response_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> When you make a DNS query, the response you get may have a DNSSEC signature, which authenticates that DNS records originate from an authorized sender, thus protecting DNS from falsified information. <br> To filter only responses signed by an authorized sender, use: The `only_dnssec_response` filter usage syntax is:<br> === \"YAML\" ```yaml only_dnssec_response: true ``` === \"JSON\" ```json { \"only_dnssec_response\": true } ``` <br> **answer_count:** *int* <a name=\"answer_count_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> One of the properties present in the query message structure is `Answer RRs`, which is the count of entries in the responses section (RR stands for \u201cresource record\u201d). <br> The number of answers in the query is always zero, as a query message has only questions and no answers, and when the server sends the answer to that query, the value is set to the amount of entries in the answers section. <br> The `answer_count` filter usage syntax is:<br> === \"YAML\" ```yaml answer_count: int ``` === \"JSON\" ```json { \"answer_count\": int } ``` with the `int` referring to the desired amount of answer. <br> Note that any value greater than zero that is defined will exclude queries from the results, since in queries the number of answers is always 0. <br> As the answers count of queries is 0, whenever the value set for the answer_count is 0, both queries and responses will compose the result. <br> A special case is the concept of `NODATA`, which is one of the possible returns to a query made to a DNS server is known as. This happens when the query is successful (so rcode:0), but there is no data as a response, so the number of answers is 0. <br> In this case, to have in the results only the cases of `NODATA`, that is, the responses, the filter must be used together with the filter `exclude_noerror`. Important information is that only one answer_count is possible for each handler. So, in order to have multiple counts on the same policy, multiple handlers must be created, each with an amount of answers. **only_qtype:** *str[]* <a name=\"only_qtype_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> DNS record types are records that provide important information about a hostname or domain. Supported default types can be seen [here](https://github.com/orb-community/pktvisor/blob/develop/libs/visor_dns/dns.h#L30). <br> The `only_qtype` filter usage syntax is:<br> === \"YAML\" ```yaml only_qtype: - str - str ``` === \"JSON\" ```json { \"only_qtype\": [ \"str\", \"str\" ] } ``` If you want to filter only IPV4 record types, for example, you should use: <br> === \"YAML\" ```yaml only_qtype: - \"A\" ``` === \"JSON\" ```json { \"only_qtype\": [ \"A\" ] } ``` or === \"YAML\" ```yaml only_qtype: - 1 ``` === \"JSON\" ```json { \"only_qtype\": [ 1 ] } ``` Multiple types are also supported and both queries and responses that have any of the values in the array will be considered. === \"YAML\" ```yaml only_qtype: - 1 - 2 - \"A\" ``` === \"JSON\" ```json { \"only_qtype\": [ 1, 2, \"A\" ] } ``` <br> **only_qname:** *str[]* <a name=\"only_qname_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> The `only_qname` filters dns packets based on queries and responses whose names exactly matches the strings present in the array. <br> The `only_qname` filter usage syntax is:<br> === \"YAML\" ```yaml only_qname: - str - str ``` === \"JSON\" ```json { \"only_qname\": [ \"str\", \"str\" ] } ``` Examples: === \"YAML\" ```yaml only_qname: - www.google.com - .nsone.net ``` === \"JSON\" ```json { \"only_qname\": [ \"www.google.com\", \".nsone.net\" ] } ``` <br> **only_qname_suffix:** *str[]* <a name=\"only_qname_suffix_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> The `only_qname_suffix` filters queries and responses whose endings (suffixes) of the names match the strings present in the array. <br> The `only_qname_suffix` filter usage syntax is:<br> === \"YAML\" ```yaml only_qname_suffix: - str - str ``` === \"JSON\" ```json { \"only_qname_suffix\": [ \"str\", \"str\" ] } ``` Examples: === \"YAML\" ```yaml only_qname_suffix: - .google.com ``` === \"JSON\" ```json { \"only_qname_suffix\": [ \".google.com\" ] } ``` or === \"YAML\" ```yaml only_qname_suffix: - google.com - .nsone.net ``` === \"JSON\" ```json { \"only_qname_suffix\": [ \"google.com\", \".nsone.net\" ] } ``` <br> **geoloc_notfound:** *bool* <a name=\"geoloc_notfound_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> Based on ECS (EDNS Client Subnet) information, it is possible to determine the geolocation of where the query is being made. When the Subnet refers to a region found in the standard databases, the city, state and country (approximated) are returned. However, when based on the subnet it is not possible to determine the geolocation, a `not found` is returned. <br> The `geoloc_notfound` filter only keeps responses whose geolocations were not found. <br> The `geoloc_notfound` filter usage syntax is:<br> === \"YAML\" ```yaml geoloc_notfound: true ``` === \"JSON\" ```json { \"geoloc_notfound\": true } ``` <br> **asn_notfound:** *bool* <a name=\"asn_notfound_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> Based on ECS (EDNS Client Subnet) information, it is possible to determine the ASN (Autonomous System Number). When the IP of the subnet belongs to some not known ASN in the standard databases, a `not found` is returned. <br> The `asn_notfound` filter only keeps responses whose asn were not found. <br> The `asn_notfound` filter usage syntax is:<br> === \"YAML\" ```yaml asn_notfound: true ``` === \"JSON\" ```json { \"asn_notfound\": true } ``` <br> **only_queries:** *bool* <a name=\"only_queries_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> The `only_queries` filters out all dns response packets and its usage syntax is:<br> === \"YAML\" ```yaml only_queries: true ``` === \"JSON\" ```json { \"only_queries\": true } ``` <br> **only_responses:** *bool* <a name=\"only_responses_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: PCAP <br> The `only_responses` filters out all dns queries packets and its usage syntax is: <br> === \"YAML\" ```yaml only_responses: true ``` === \"JSON\" ```json { \"only_responses\": true } ``` <br> **dnstap_msg_type:** *str* <a name=\"dnstap_msg_type_v1\"></a><br> <font size=\"1\">[Back to DNS-v1 filters list](#dns_filters_v1)</font> Input: DNSTAP <br> With a dnstap protocol it is possible to know the type of message that must be resolved in the request to the server. This filter therefore allows you to filter by response types. Supported message types are: `auth`, `resolver`, `client`, `forwarder`, `stub`, `tool` and `update`. The `dnstap_msg_type` filter usage syntax is:<br> === \"YAML\" ```yaml dnstap_msg_type: str ``` Example: ```yaml dnstap_msg_type: auth ``` === \"JSON\" ```json { \"dnstap_msg_type\": \"str\" } ``` Example: ```json { \"dnstap_msg_type\": \"auth\" } ``` <br> Configurations public_suffix_list : bool . recorded_stream : bool . xact_ttl_secs : int . xact_ttl_ms : int . Abstract configurations . public_suffix_list Back to DNS configurations list Some names to be resolved by a dns server have public suffixes. These suffixes cause metrics to be generated considering non-relevant data. The example below illustrates the benefit of using this type of configuration. The qnames consider each part of the name to be resolved. When a name has a public suffix, generic information is generated. Note that in the standard configuration, Qname2 and Qname3 are the same for both domains. With the public suffix setting true (which makes the entire public part be considered as a single part), Qname3 already displays relevant information about the name. The list of suffixes considered public can be accessed here . Name Qname2 Standard Qname3 Standard Qname2 Public Suffix Qname3 Public Suffix www.imagine.qname.co.uk co.uk qname.co.uk qname.co.uk imagine.qname.co.uk other.example.qname.co.uk co.uk qname.co.uk qname.co.uk example.qname.co.uk The public_suffix_list configuration usage syntax is: YAML JSON public_suffix_list : true { \"public_suffix_list\" : true } recorded_stream Back to DNS configurations list This configuration is useful when a pcap_file is used in taps/input configuration. Set it to True when you want to load an offline traffic (from a pcap_file). The recorded_stream configuration usage syntax is: YAML JSON recorded_stream : true { \"recorded_stream\" : true } xact_ttl_ms OR xact_ttl_secs Back to DNS configurations list Both configurations have the same functionality, that is, defines the time to live of transactions, and only change the unit of measurement to be configured. This configuration causes the metrics to be generated for complete transactions (query and response) within the established time limit. xact_ttl_ms: Defines the time to live of transactions in milliseconds. xact_ttl_secs: Defines the time to live of transactions in seconds. Note that xact_ttl_ms is dominant over xact_ttl_secs , that is, if xact_ttl_ms exists, even if xact_ttl_secs also exists, the value of xact_ttl_ms will be considered. The xact_ttl_ms or xact_ttl_secs configuration usage syntax is: YAML JSON xact_ttl_ms : 5000 or xact_ttl_secs : 5 { \"xact_ttl_ms\" : 5000 } or { \"xact_ttl_secs\" : 5 } Network (L2-L3) Analyzer (net) Example of policy Metrics Group Filters Configurations NET(v2) NET(v1) Warning Status: Beta . The metric names and configuration options may still change Example of policy with input pcap and handler NET(v2) YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 5 topn_count : 10 modules : default_net : type : net require_version : \"2.0\" config : deep_sample_rate : 1 num_periods : 2 topn_count : 25 filter : geoloc_notfound : true asn_notfound : true only_geoloc_prefix : - BR - US/CA only_asn_number : - 7326 - 16136 metric_groups : disable : - cardinality - counters - top_geo - top_ips - quantiles input : input_type : pcap tap_selector : any : - key1 : value1 - key2 : value2 filter : bpf : net 192.168.1.0/24 config : iface : wlo1 host_spec : 192.168.1.0/24 pcap_source : libpcap debug : true kind : collection { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 5 , \"topn_count\" : 10 }, \"modules\" : { \"default_net\" : { \"type\" : \"net\" , \"require_version\" : \"2.0\" , \"config\" : { \"deep_sample_rate\" : 1 , \"num_periods\" : 2 , \"topn_count\" : 25 }, \"filter\" : { \"geoloc_notfound\" : true , \"asn_notfound\" : true , \"only_geoloc_prefix\" : [ \"BR\" , \"US/CA\" ], \"only_asn_number\" : [ 7326 , 16136 ] }, \"metric_groups\" : { \"disable\" : [ \"cardinality\" , \"counters\" , \"top_geo\" , \"top_ips\" , \"quantiles\" ] } } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap_selector\" : { \"any\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ] }, \"filter\" : { \"bpf\" : \"net 192.168.1.0/24\" }, \"config\" : { \"iface\" : \"wlo1\" , \"host_spec\" : \"192.168.1.0/24\" , \"pcap_source\" : \"libpcap\" , \"debug\" : true } }, \"kind\" : \"collection\" } Handler Type : \"net\" Metrics Group - Check the net metrics belonging to each group Metric Group Default cardinality enabled counters enabled top_geo enabled top_ips enabled quantiles enabled Filters Filter Type Input geoloc_notfound bool PCAP, DNSTAP asn_notfound bool PCAP, DNSTAP only_geoloc_prefix str[] PCAP, DNSTAP only_asn_number str[] PCAP, DNSTAP geoloc_notfound: bool Back to net-v2 filters list Input: PCAP The source and destination IPs are used to determine the geolocation to know where the data is from and where it is going. When the IPs refer to a region found in the standard databases, the city, state and country (approximated) are returned. However, when it is not possible to determine the IP geolocation, a not found is returned. The geoloc_notfound filter usage syntax is: YAML JSON geoloc_notfound : true { \"geoloc_notfound\" : true } asn_notfound: bool Back to net-v2 filters list Input: PCAP Based on source and destination IP, it is possible to determine the ASN (Autonomous System Number). When the IP of the source or destination belongs to some not known ASN in the standard databases, a not found is returned. The asn_notfound filter usage syntax is: YAML JSON asn_notfound : true { \"asn_notfound\" : true } only_geoloc_prefix: str[] Back to net-v2 filters list Input: PCAP Source and destination IPs are used to determine the geolocation to know where the data is from and where it is going. In this way it is possible to filter the data considering the geolocation using the filter only_geoloc_prefix . The only_geoloc_prefix filter usage syntax is: YAML JSON only_geoloc_prefix : - str - str Example: only_geoloc_prefix : - BR - US/CA { \"only_geoloc_prefix\" : [ \"str\" , \"str\" ] } Example: { \"only_geoloc_prefix\" : [ \"BR\" , \"US/CA\" ] } only_asn_number: str[] Back to net-v2 filters list Input: PCAP Based on source and destination IP, it is possible to determine the ASN (Autonomous System Number). In this way it is possible to filter the data considering a specific ASN using the filter only_asn_number . The only_asn_number filter usage syntax is: YAML JSON only_asn_number : - str - str Example: only_asn_number : - 7326 - 16136 { \"only_asn_number\" : [ \"str\" , \"str\" ] } Example: { \"only_asn_number\" : [ 7326 , 16136 ] } Example of policy with input pcap and handler NET(v1) YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 5 topn_count : 10 modules : default_net : type : net config : deep_sample_rate : 1 num_periods : 2 topn_count : 25 filter : geoloc_notfound : true asn_notfound : true only_geoloc_prefix : - BR - US/CA only_asn_number : - 7326 - 16136 metric_groups : disable : - cardinality - counters - top_geo - top_ips input : input_type : pcap tap_selector : any : - key1 : value1 - key2 : value2 filter : bpf : net 192.168.1.0/24 config : iface : wlo1 host_spec : 192.168.1.0/24 pcap_source : libpcap debug : true kind : collection { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 5 , \"topn_count\" : 10 }, \"modules\" : { \"default_net\" : { \"type\" : \"net\" , \"config\" : { \"deep_sample_rate\" : 1 , \"num_periods\" : 2 , \"topn_count\" : 25 }, \"filter\" : { \"geoloc_notfound\" : true , \"asn_notfound\" : true , \"only_geoloc_prefix\" : [ \"BR\" , \"US/CA\" ], \"only_asn_number\" : [ 7326 , 16136 ] }, \"metric_groups\" : { \"disable\" : [ \"cardinality\" , \"counters\" , \"top_geo\" , \"top_ips\" ] } } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap_selector\" : { \"any\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ] }, \"filter\" : { \"bpf\" : \"net 192.168.1.0/24\" }, \"config\" : { \"iface\" : \"wlo1\" , \"host_spec\" : \"192.168.1.0/24\" , \"pcap_source\" : \"libpcap\" , \"debug\" : true } }, \"kind\" : \"collection\" } Handler Type : \"net\" Metrics Group - Check the net metrics belonging to each group Metric Group Default cardinality enabled counters enabled top_geo enabled top_ips enabled Filters Filter Type Input geoloc_notfound bool PCAP, DNSTAP asn_notfound bool PCAP, DNSTAP only_geoloc_prefix str[] PCAP, DNSTAP only_asn_number str[] PCAP, DNSTAP geoloc_notfound: bool Back to net-v1 filters list Input: PCAP The source and destination IPs are used to determine the geolocation to know where the data is from and where it is going. When the IPs refer to a region found in the standard databases, the city, state and country (approximated) are returned. However, when it is not possible to determine the IP geolocation, a not found is returned. The geoloc_notfound filter usage syntax is: YAML JSON geoloc_notfound : true { \"geoloc_notfound\" : true } asn_notfound: bool Back to net-v1 filters list Input: PCAP Based on source and destination IP, it is possible to determine the ASN (Autonomous System Number). When the IP of the source or destination belongs to some not known ASN in the standard databases, a not found is returned. The asn_notfound filter usage syntax is: YAML JSON asn_notfound : true { \"asn_notfound\" : true } only_geoloc_prefix: str[] Back to net-v1 filters list Input: PCAP Source and destination IPs are used to determine the geolocation to know where the data is from and where it is going. In this way it is possible to filter the data using the geolocation using the filter only_geoloc_prefix . . The filter supports the following strings: Continents: two-character continent code, as follows: AF - Africa AN - Antarctica AS - Asia EU - Europe NA - North America OC - Oceania SA - South America Country: the two-character ISO 3166-1 country code Subdivision: the region-portion of the ISO 3166-2 code for the region The only_geoloc_prefix filter usage syntax is: YAML JSON only_geoloc_prefix : - str - str Example: only_geoloc_prefix : - BR - US/CA { \"only_geoloc_prefix\" : [ \"str\" , \"str\" ] } Example: { \"only_geoloc_prefix\" : [ \"BR\" , \"US/CA\" ] } only_asn_number: str[] Back to net-v1 filters list Input: PCAP Based on source and destination IP, it is possible to determine the ASN (Autonomous System Number). In this way it is possible to filter the data considering a specific ASN using the filter only_asn_number . The only_asn_number filter usage syntax is: YAML JSON only_asn_number : - str - str Example: only_asn_number : - 7326 - 16136 { \"only_asn_number\" : [ \"str\" , \"str\" ] } Example: { \"only_asn_number\" : [ 7326 , 16136 ] } Configurations recorded_stream : bool . Abstract configurations . recorded_stream Back to net configurations list This configuration is useful when a pcap_file is used in taps/input configuration. Set it to True when you want to load an offline traffic (from a pcap_file). The recorded_stream configuration usage syntax is: YAML JSON recorded_stream : true { \"recorded_stream\" : true } DHCP Analyzer (dhcp) Example of policy Metrics Group Filters Configurations Example of policy with input pcap and handler DHCP YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 8 topn_count : 10 modules : default_dhcp : type : dhcp config : deep_sample_rate : 1 num_periods : 8 topn_count : 25 input : input_type : pcap tap_selector : all : - key1 : value1 - key2 : value filter : bpf : net 192.168.1.0/24 config : iface : wlo1 host_spec : 192.168.1.0/24 pcap_source : libpcap debug : true kind : collection { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 8 , \"topn_count\" : 10 }, \"modules\" : { \"default_dhcp\" : { \"type\" : \"dhcp\" , \"config\" : { \"deep_sample_rate\" : 1 , \"num_periods\" : 8 , \"topn_count\" : 25 } } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap_selector\" : { \"all\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value\" } ] }, \"filter\" : { \"bpf\" : \"net 192.168.1.0/24\" }, \"config\" : { \"iface\" : \"wlo1\" , \"host_spec\" : \"192.168.1.0/24\" , \"pcap_source\" : \"libpcap\" , \"debug\" : true } }, \"kind\" : \"collection\" } Handler Type : \"dhcp\" Metrics Group - Check dhcp metrics No metrics group available Filters No filters available. Configurations Abstract configurations . BGP Analyzer (bgp) Example of policy Metrics Group Filters Configurations Example of policy with input pcap and handler BGP YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 8 topn_count : 10 modules : default_bgp : type : bgp config : deep_sample_rate : 1 num_periods : 8 topn_count : 25 input : input_type : pcap tap_selector : all : - key1 : value1 - key2 : value filter : bpf : net 192.168.1.0/24 config : iface : wlo1 host_spec : 192.168.1.0/24 pcap_source : libpcap debug : true config : merge_like_handlers : true kind : collection { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 8 , \"topn_count\" : 10 }, \"modules\" : { \"default_bgp\" : { \"type\" : \"bgp\" , \"config\" : { \"deep_sample_rate\" : 1 , \"num_periods\" : 8 , \"topn_count\" : 25 } } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap_selector\" : { \"all\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value\" } ] }, \"filter\" : { \"bpf\" : \"net 192.168.1.0/24\" }, \"config\" : { \"iface\" : \"wlo1\" , \"host_spec\" : \"192.168.1.0/24\" , \"pcap_source\" : \"libpcap\" , \"debug\" : true } }, \"config\" : { \"merge_like_handlers\" : true }, \"kind\" : \"collection\" } Handler Type : \"bgp\" Metrics Group - Check BGP metrics No metrics group available Filters No filters available. Configurations Abstract configurations . Packet Capture Analyzer (pcap) Example of policy Metrics Group Filters Configurations Example of policy with input pcap and handler PCAP YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 8 topn_count : 10 modules : default_pcap : type : pcap config : deep_sample_rate : 6 num_periods : 3 topn_count : 25 input : input_type : pcap tap : default_pcap filter : bpf : net 192.168.1.0/24 config : iface : wlo1 host_spec : 192.168.1.0/24 pcap_source : libpcap debug : true kind : collection { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 8 , \"topn_count\" : 10 }, \"modules\" : { \"default_pcap\" : { \"type\" : \"pcap\" , \"config\" : { \"deep_sample_rate\" : 6 , \"num_periods\" : 3 , \"topn_count\" : 25 } } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"default_pcap\" , \"filter\" : { \"bpf\" : \"net 192.168.1.0/24\" }, \"config\" : { \"iface\" : \"wlo1\" , \"host_spec\" : \"192.168.1.0/24\" , \"pcap_source\" : \"libpcap\" , \"debug\" : true } }, \"kind\" : \"collection\" } Handler Type : \"pcap\" Metrics Group - Check pcap metrics No metrics group available. Filters No filters available. Configurations Abstract configurations . Flow Analyzer (flow) [BETA] Warning Status: Beta . The metric names and configuration options may still change Example of policy Metrics Group Filters Configurations Example of policy with input flow and handler FLOW YAML JSON handlers : config : deep_sample_rate : 95 num_periods : 6 topn_count : 8 modules : my_flow : type : flow config : sample_rate_scaling : false deep_sample_rate : 85 num_periods : 5 topn_count : 7 first_filter_if_as_label : true enrichment : true device_map : 192.168.3.32 : name : Device1 description : This is a device map example interfaces : 2 : name : Prov1 description : This is an interface map example summarize_ips_by_asn : true exclude_unknown_asns_from_summarization : true exclude_asns_from_summarization : - 16509 exclude_ips_from_summarization_flow : - 192.168.3.32/32 metric_groups : enable : - cardinality - counters - top_geo - by_packets - by_bytes - conversations - top_ports - top_ips - top_interfaces - top_ips_ports - top_tos filter : only_ports : - 10853 - 10860-10890 only_device_interfaces : - 216.239.38.10 : - 2 only_directions : \"in\" only_ips : - 192.168.2.1/24 - 192.158.1.38/32 geoloc_notfound : true asn_notfound : true input : input_type : flow tap : default_flow kind : collection { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 95 , \"num_periods\" : 6 , \"topn_count\" : 8 }, \"modules\" : { \"my_flow\" : { \"type\" : \"flow\" , \"config\" : { \"sample_rate_scaling\" : false , \"deep_sample_rate\" : 85 , \"num_periods\" : 5 , \"topn_count\" : 7 , \"first_filter_if_as_label\" : true , \"enrichment\" : true , \"device_map\" : { \"192.168.3.32\" : { \"name\" : \"Device1\" , \"description\" : \"This is a device map example\" , \"interfaces\" : { \"2\" : { \"name\" : \"Prov1\" , \"description\" : \"This is an interface map example\" } } } }, \"summarize_ips_by_asn\" : true , \"exclude_unknown_asns_from_summarization\" : true , \"exclude_asns_from_summarization\" : [ 16509 ], \"exclude_ips_from_summarization_flow\" : [ \"192.168.3.32/32\" ] }, \"metric_groups\" : { \"enable\" : [ \"cardinality\" , \"counters\" , \"top_geo\" , \"by_packets\" , \"by_bytes\" , \"conversations\" , \"top_ports\" , \"top_ips\" , \"top_interfaces\" , \"top_ips_ports\" , \"top_tos\" ] }, \"filter\" : { \"only_ports\" : [ 10853 , \"10860-10890\" ], \"only_device_interfaces\" : [ { \"216.239.38.10\" : [ 2 ] } ], \"only_directions\" : \"in\" , \"only_ips\" : [ \"192.168.2.1/24\" , \"192.158.1.38/32\" ], \"geoloc_notfound\" : true , \"asn_notfound\" : true } } } }, \"input\" : { \"input_type\" : \"flow\" , \"tap\" : \"default_flow\" }, \"kind\" : \"collection\" } Handler Type : \"flow\" Metrics Group - Check the flow metrics belonging to each group Metric Group Default cardinality enabled counters enabled by_packets enabled by_bytes enabled top_ips enabled top_ports enabled top_ips_ports enabled top_geo disabled conversations disabled top_interfaces disabled top_tos disabled Filters Filter Type Input only_device_interfaces str[] FLOW only_directions str FLOW only_ips str[] FLOW only_ports str[] FLOW geoloc_notfound bool FLOW asn_notfound bool FLOW only_device_interfaces: str[] Back to flow filters list Input: FLOW only_device_interfaces filters data by only retaining flows coming from the specific devices and interfaces defined in this filter. The difference between only_device_interfaces and only_ips is that only_ips filters based on the IPs observed inside the flows, while only_device_interfaces filters based on the device and interface sending the flows. The only_device_interfaces filter usage syntax is: YAML JSON only_device_interfaces : - device : - interface Example: only_device_interfaces : - 216.239.38.10 : - 2 #port can be passed as int - 4-10 #port can be passed as range. Ports from 4 to 10: all ports in this interval will be accepted. - \"1\" #port can be passed as str - 192.158.1.38 : [ 9 , 4-10 ] - 192.168.2.32 : - \"*\" #all ports { \"only_device_interfaces\" : [ { \"device\" : [ \"interface\" ] } ] } Example: { \"only_device_interfaces\" : [ { \"216.239.38.10\" : [ 2 , \"4-10\" , \"10853\" ] }, { \"192.158.1.38\" : [ 3 , \"10860-10890\" ] }, { \"192.168.2.32\" : [ \"*\" ] } ] } only_directions: str[] Back to flow filters list only_directions filters data by its direction. Options are: \"in\" and/or \"out\". The only_directions filter usage syntax is: YAML JSON only_directions : [ str ] Example: only_directions : - \"in\" { \"only_directions\" : [ \"str\" ] } Example: { \"only_directions\" : [ \"in\" ] } only_ips: str[] Back to flow filters list Input: FLOW To filter data only from certain source OR destination, you can use only_ips filter, for which CIDR (Inter-Domain Routing Classes) ranges are supported. The only_ips filter usage syntax is: YAML JSON only_ips : - array Example: only_ips : - 192.168.1.1/24 - 192.158.1.38/32 { \"only_ips\" : [ \"array\" ] } Example: { \"only_ips\" : [ \"192.168.2.1/24\" , \"192.158.1.38/32\" ] } only_ports: str[] Back to flow filters list Input: FLOW only_ports filter only filters data being sent to or received on one of the selected TCP/UDP ports (or range of ports). The only_ports filter usage syntax is: YAML JSON only_ports : - array Example: only_ports : - 10853 #port can be passed as int - \"10854\" #port can be passed as str - 10860-10890 #range from 10860 to 10890. All ports in this interval will be accepted { \"only_ports\" : [ \"array\" ] } Example: { \"only_ports\" : [ 10853 , \"10854\" , \"10860-10890\" ] } geoloc_notfound: bool Back to flow filters list Input: FLOW The source and destination IPs are used to determine the geolocation to know where the data is from and where it is going. When the IPs refer to a region found in the standard databases, the city, state and country (approximated) are returned. However, when it is not possible to determine the IP geolocation, a not found is returned. The geoloc_notfound filter usage syntax is: YAML JSON geoloc_notfound : true { \"geoloc_notfound\" : true } asn_notfound: bool Back to flow filters list Input: FLOW Based on source and destination IP, it is possible to determine the ASN (Autonomous System Number). When the IP of the source or destination belongs to some not known ASN in the standard databases, a not found is returned. The asn_notfound filter usage syntax is: YAML JSON asn_notfound : true { \"asn_notfound\" : true } Configurations sample_rate_scaling : bool first_filter_if_as_label : bool enrichment : bool device_map : map summarize_ips_by_asn : bool exclude_asns_from_summarization : str[] exclude_unknown_asns_from_summarization : bool subnets_for_summarization : str[] exclude_ips_from_summarization str[] recorded_stream : bool Abstract configurations . sample_rate_scaling Back to flow configurations list By default, flow metrics are generated by an approximation based on sampling the data. 1 packet every N is analyzed and the prediction of the entire population is made from the sample. If you want to see exactly all the exact data, you can disable sample_rate_scaling . The sample_rate_scaling filter usage syntax is: YAML JSON sample_rate_scaling : false { \"sample_rate_scaling\" : false } first_filter_if_as_label Back to flow configurations list This configuration requires the only_interfaces filter to be active (true). If this setting is true , the interfaces will be used as labels for the metrics. The first_filter_if_as_label filter usage syntax is: YAML JSON first_filter_if_as_label : true { \"first_filter_if_as_label\" : true } enrichment Back to flow configurations list When true, uses device map settings. The enrichment configuration usage syntax is: YAML JSON enrichment : True { \"enrichment\" : true } device_map Back to flow configurations list This configuration allows the user to assign a custom name to devices and interfaces, and the proper functioning of this configuration depends on the enrichment being True. The device_map configuration usage syntax is: YAML JSON device_map : device_ip : name : \"str\" #name is required description : \"Optionally set a description\" interfaces : #Interfaces are optional interface : name : \"str\" #required description : \"Optionally set a description\" Example: device_map : 192.168.2.32 : name : Cisco description : This is a device map example interfaces : 2 : name : GoogleProv description : This is an interface map example { \"device_map\" : { \"device_ip\" : { \"name\" : \"str\" , \"description\" : \"Optionally set a description\" , \"interfaces\" : { \"interface\" : { \"name\" : \"str\" , \"description\" : \"Optionally set a description\" } } } } } Example: { \"device_map\" : { \"192.168.2.32\" : { \"name\" : \"Cisco\" , \"description\" : \"This is a device map example\" , \"interfaces\" : { \"2\" : { \"name\" : \"GoogleProv\" , \"description\" : \"This is an interface map example\" } } } } } Summarization is a useful strategy for visualization, but also for decreasing the cardinality of the data, and two types of summarization are supported: by asn and by subnets , and summarization by ASN is dominant over subnet, i.e. If both configurations are present, only the IPs of an excluded asn or an unknown asn (if exclude_unknown_asns_from_summarization is true) will be summarized by subnet. summarize_ips_by_asn Back to flow configurations list When True, it summarizes data by ASN (Autonomous System Number). The summarize_ips_by_asn configuration usage syntax is: YAML JSON summarize_ips_by_asn : true { \"summarize_ips_by_asn\" : true } exclude_asns_from_summarization Back to flow configurations list This configuration must be used in conjunction with summarize_ips_by_asn , in order to exclude ASNs from summarization. In this case, packets transacted by excluded ASNs will be exposed by IPs. The exclude_asns_from_summarization configuration usage syntax is: YAML JSON exclude_asns_from_summarization : - 8075 - 16509 { \"exclude_asns_from_summarization\" : [ 8075 , 16509 ] } exclude_unknown_asns_from_summarization Back to flow configurations list This configuration must be used in conjunction with summarize_ips_by_asn , in order to expose IPs from packets transacted by unknown ASNs. The exclude_unknown_asns_from_summarization configuration usage syntax is: YAML JSON exclude_unknown_asns_from_summarization : true { \"exclude_unknown_asns_from_summarization\" : true } subnets_for_summarization Back to flow configurations list This configuration allows the summarization of flow data by subnets. Attention: This configuration will only work properly if the summarize_ips_by_asn configuration is not set for the IP, since summarize_ips_by_asn is dominant. The subnets_for_summarization configuration usage syntax is: YAML JSON subnets_for_summarization : - 192.168.2.1/24 { \"subnets_for_summarization\" : [ \"192.168.2.1/24\" ] } Tip It is possible to define a summary pattern by defining the CIDR only. In this way, all present IPs will be summarized. For this to be done, just pass the default host mask for IPv4 or/and IPv6 and the desired CIDR for grouping. This pattern has less priority than the explicitly defined subnets, so only IPs not belonging to any set subnet will be summarized following the general pattern. YAML JSON subnets_for_summarization : - 0.0.0.0/16 - ::/64 { \"subnets_for_summarization\" : [ \"0.0.0.0/16\" , \"::/64\" ] } exclude_ips_from_summarization_flow Back to flow configurations list This configuration must be used in conjunction with summarize_ips_by_asn or subnets_for_summarization and will remove the specified IPs from the summarization. The exclude_ips_from_summarization_flow configuration usage syntax is: YAML JSON exclude_ips_from_summarization_flow : - 192.168.2.1/31 { \"exclude_ips_from_summarization_flow\" : [ \"192.168.2.1/31\" ] } recorded_stream Back to flow configurations list This configuration is useful when a pcap_file is used in taps/input configuration. Set it to True when you want to load an offline traffic (from a pcap_file). The recorded_stream configuration usage syntax is: YAML JSON recorded_stream : true { \"recorded_stream\" : true } Netprobe [BETA] Warning Status: Beta . The metric names and configuration options may still change Example of policy Metrics Group Filters Configurations Example of policy with input netprobe and handler NETPROBE YAML JSON handlers : modules : default_netprobe : type : netprobe metric_groups : enable : - counters - quantiles - histograms config : targets : www.google.com : target : www.google.com orb_community : target : orb.community input : input_type : netprobe tap : default_netprobe kind : collection { \"handlers\" : { \"modules\" : { \"default_netprobe\" : { \"type\" : \"netprobe\" , \"metric_groups\" : { \"enable\" : [ \"counters\" , \"quantiles\" , \"histograms\" ] }, \"config\" : { \"targets\" : { \"www.google.com\" : { \"target\" : \"www.google.com\" }, \"orb_community\" : { \"target\" : \"orb.community\" } } } } } }, \"input\" : { \"input_type\" : \"netprobe\" , \"tap\" : \"default_netprobe\" }, \"kind\" : \"collection\" } Handler Type : \"netprobe\" Metrics Group - Check the netprobe metrics belonging to each group Metric Group Default quantiles disabled counters enabled histograms enabled Filters No filters available. Configurations Abstract configurations . Config Type Required Default targets map \u2705 - targets : map Back to netprobe configurations list Here, the targets against which the probe will run are defined. For each target is required to specify the target name and the address to be tested. YAML targets : map Example: targets : target_name : target : ipv4 address to test Generic Example: targets : google : target : www.google.com In netprobe policies it makes a lot of sense to use the settings from the input directly in the policy, since the settings are more related to the probe than the device the orb agent is running on. Therefore, it is worth reinforcing here the ability to override all tap settings in the policy. See here the available configurations for netprobe. YAML JSON handlers : modules : default_netprobe : type : netprobe metric_groups : enable : - counters - quantiles - histograms config : targets : www.google.com : target : www.google.com orb_community : target : orb.community input : input_type : netprobe tap : default_netprobe config : test_type : ping interval_msec : 2500 timeout_msec : 2000 packets_per_test : 5 packets_interval_msec : 20 packet_payload_size : 56 kind : collection { \"handlers\" : { \"modules\" : { \"default_netprobe\" : { \"type\" : \"netprobe\" , \"metric_groups\" : { \"enable\" : [ \"counters\" , \"quantiles\" , \"histograms\" ] }, \"config\" : { \"targets\" : { \"www.google.com\" : { \"target\" : \"www.google.com\" }, \"orb_community\" : { \"target\" : \"orb.community\" } } } } } }, \"input\" : { \"input_type\" : \"netprobe\" , \"tap\" : \"default_netprobe\" , \"config\" : { \"test_type\" : \"ping\" , \"interval_msec\" : 2500 , \"timeout_msec\" : 2000 , \"packets_per_test\" : 5 , \"packets_interval_msec\" : 20 , \"packet_payload_size\" : 56 } }, \"kind\" : \"collection\" }","title":"Orb Policy Reference"},{"location":"documentation/advanced_policies/#orb-policy-reference","text":"The policy model has seven top level sections: name , desciption , tags , backend , schema_version , policy and format . JSON { \"name\" : \"\" , \"description\" : \"\" , \"tags\" : {}, \"backend\" : \"\" , \"schema_version\" : \"\" , \"format\" : \"\" , \"policy\" : {} } Policy Sections Default Required name - \u2705 backend - \u2705 policy - \u2705 description None \u274c tags None \u274c schema_version 1.0 \u274c format json \u274c Name Policy name must be unique, must start with a letter and contain only letters, numbers, \"-\" or \"_\" . Examples Success \"name\": \"my_policy\" \"name\": \"MY-policy_2 Failure \"name\": \"1_policy\" \"name\": \"MY-policy/2 Description Policy description is a free string. You can describe using spaces, letters, numbers and special characters. Tags Tags are intended to facilitate organization and are a dict type. The tags usage syntax is: JSON \"tags\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" , \"key3\" : \"value3\" } Backend Backend determine to which backend the policy will be attached. Check the available backends here . Schema version Each backend supported on Orb must have a policy schema to be validated, parsed and applied. schema_version is the field responsible for allowing different schema versions and backward compatibility. Default should be \"1.0\". Format The format specifies in which format the policy data will be written. The options are json and yaml . json is the default value. Policy Each supported backend has specific structures for its policy data. Check the available options below.","title":"Orb Policy Reference"},{"location":"documentation/advanced_policies/#pktvisor-policy","text":"The policy data for pktvisor can be written in either YAML or JSON, and has four top level sections: \u201cinput\u201d, \u201chandlers\u201d, \"config\" and \u201ckind\u201d. YAML JSON input : .. config : ... kind : collection handlers : ... { \"input\" : { }, \"config\" : { }, \"kind\" : \"collection\" }, \"handlers\" : { }","title":"Pktvisor policy"},{"location":"documentation/advanced_policies/#input-section","text":"The input section specifies what data streams the policy will be using for analysis, in other words, this specifies what data the agent should be listening in on, and is defined at the agent level . Required fields: input_type - the type of input. This field will be validated with the type of tap indicated by the tap parameter or by the tap selector . If the types are incompatible, the policy will fail. tap - the name given to this input in the tap/agent configuration or tap_selector - tags to match existing agent taps . If tap_selector is used, it can be chosen whether taps with any of the tags or with all tags will be attached. Optional fields: filter - to specify what data to include from the input config - how the input will be used Note Every configuration set at the input can be reset at the tap level, with the one set on the tap dominant over the one set on the input.","title":"Input section"},{"location":"documentation/advanced_policies/#default-input-structure","text":"Using specific tap ( check the application in a policy here ): YAML JSON input : tap : tap_name input_type : type_of_input filter : bpf : ... config : ... { \"input\" : { \"tap\" : \"tap_name\" , \"input_type\" : \"type_of_input\" , \"filter\" : { \"bpf\" : \"...\" }, \"config\" : \"...\" } } or using tap selector matching any ( check the application in a policy here ): YAML JSON input : tap_selector : any : - key1 : value1 - key2 : value2 input_type : type_of_input filter : bpf : ... config : ... { \"input\" : { \"tap_selector\" : { \"any\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ] }, \"input_type\" : \"type_of_input\" , \"filter\" : { \"bpf\" : \"...\" }, \"config\" : \"...\" } } or using tap selector matching all ( check the application in a policy here ): YAML JSON input : tap_selector : all : - key1 : value1 - key2 : value2 input_type : type_of_input filter : bpf : ... config : ... { \"input\" : { \"tap_selector\" : { \"all\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ] }, \"input_type\" : \"type_of_input\" , \"filter\" : { \"bpf\" : \"...\" }, \"config\" : \"...\" } }","title":"Default input structure"},{"location":"documentation/advanced_policies/#config-section","text":"There is the possibility of defining settings on the policy level. Currently, the only configuration available is the merge_like_handlers . Policy Configuration Type Default merge_like_handlers bool false","title":"Config section"},{"location":"documentation/advanced_policies/#kind-section","text":"What kind of object you want to create The only option for now is \"collection\" .","title":"Kind section"},{"location":"documentation/advanced_policies/#handlers-section-analysis","text":"Handlers are the modules responsible for extracting metrics from inputs. For each handler type, specific configuration, filters and group of metrics can be defined, and there are also configs (abstract configuration) that can be applied to all handlers: Default handler structure: YAML JSON handlers : config : deep_sample_rate : 100 num_periods : 5 topn_count : 10 topn_percentile_threshold : 0 modules : tap_name : type : ... require_version : ... config : ... filter : ... metric_groups : enable : - ... - .... disable : - ..... - ...... { \"handlers\" : { \"config\" : { \"deep_sample_rate\" : 100 , \"num_periods\" : 5 , \"topn_count\" : 10 , \"topn_percentile_threshold\" : 0 }, \"modules\" : { \"tap_name\" : { \"type\" : \"...\" , \"require_version\" : \"...\" , \"config\" : \"...\" , \"filter\" : \"...\" , \"metric_groups\" : { \"enable\" : [ \"...\" , \"....\" ], \"disable\" : [ \".....\" , \"......\" ] } } } } } To enable any metric group use the syntax: YAML JSON metric_groups : enable : - group_to_enable { \"metric_groups\" : { \"enable\" : [ \"group_to_enable\" ] } } To enable all available metric groups use the syntax: YAML JSON metric_groups : enable : - all { \"metric_groups\" : { \"enable\" : [ \"all\" ] } } In order to disable any metric group use the syntax: YAML JSON metric_groups : disable : - group_to_disable { \"metric_groups\" : { \"disable\" : [ \"group_to_disable\" ] } } To disable all metric groups use the syntax: YAML JSON metric_groups : disable : - all { \"metric_groups\" : { \"disable\" : [ \"all\" ] } } Attention: enable is dominant over disable. So if both are passed, the metrics group will be enabled;","title":"Handlers section (Analysis)"},{"location":"documentation/advanced_policies/#abstract-configurations","text":"There are general configurations, which can be applied to all handlers. These settings can be reset for each module, within the specific module configs. In this case, the configuration inside the module will override the configuration passed in general handler. Abstract Configuration Type Default deep_sample_rate int 100 (per second) num_periods int 5 topn_count int 10 topn_percentile_threshold int 0 deep_sample_rate Back to Top - Abstract Configurations deep_sample_rate determines the number of data packets that will be analyzed deeply per second. Some metrics are operationally expensive to generate, such as metrics that require string parsing (qname2, qtype, etc.). For this reason, a maximum number of packets per second to be analyzed is determined. If in one second fewer packages than the maximum amount are transacted, all packages will compose the deep metrics sample, if there are more packages than the established one, the value of the variable will be used. Allowed values are in the range [1,100]. Default value is 100. Note If a value less than 1 is passed, the deep_sample_rate will be 1. If the value passed is more than 100, deep_sample_rate will be 100. The deep_sample_rate usage syntax is: YAML JSON deep_sample_rate : int { \"deep_sample_rate\" : i nt } num_periods Back to Top - Abstract Configurations num_periods determines the amount of minutes of data that will be available on the metrics endpoint. Allowed values are in the range [2,10]. Default value is 5. The num_periods usage syntax is: YAML JSON num_periods : int { \"num_periods\" : i nt } topn_count Back to Top - Abstract Configurations topn_count sets the maximum amount of elements displayed in top metrics. If there is less quantity than the configured value, the composite metrics will have the existing value. But if there are more metrics than the configured value, the variable will be actively limiting. Any positive integer is valid and the default value is 10. The topn_count usage syntax is: YAML JSON topn_count : int { \"topn_count\" : i nt } topn_percentile_threshold Back to Top - Abstract Configurations topn_percentile_threshold sets the threshold of data to be considered based on the percentiles, so allowed values are in the range [0,100]. The default value is 0, that is, all data is considered. If, for example, the value 10 is set, scraped topn metrics will only consider data from the 10th percentile, that is, data between the highest 90%. The topn_percentile_threshold usage syntax is: YAML JSON topn_percentile_threshold : int { \"topn_percentile_threshold\" : i nt }","title":"Abstract Configurations"},{"location":"documentation/advanced_policies/#dns-analyzer-dns","text":"Example of policy Metrics Group Filters Configurations DNS(v2) DNS(v1) Warning Status: Beta . The metric names and configuration options may still change","title":"DNS Analyzer (dns)"},{"location":"documentation/advanced_policies/#network-l2-l3-analyzer-net","text":"Example of policy Metrics Group Filters Configurations NET(v2) NET(v1) Warning Status: Beta . The metric names and configuration options may still change","title":"Network (L2-L3) Analyzer (net)"},{"location":"documentation/advanced_policies/#dhcp-analyzer-dhcp","text":"Example of policy Metrics Group Filters Configurations","title":"DHCP Analyzer (dhcp)"},{"location":"documentation/advanced_policies/#bgp-analyzer-bgp","text":"Example of policy Metrics Group Filters Configurations","title":"BGP Analyzer (bgp)"},{"location":"documentation/advanced_policies/#packet-capture-analyzer-pcap","text":"Example of policy Metrics Group Filters Configurations","title":"Packet Capture Analyzer (pcap)"},{"location":"documentation/advanced_policies/#flow-analyzer-flow-beta","text":"Warning Status: Beta . The metric names and configuration options may still change Example of policy Metrics Group Filters Configurations","title":"Flow Analyzer (flow) [BETA]"},{"location":"documentation/advanced_policies/#netprobe-beta","text":"Warning Status: Beta . The metric names and configuration options may still change Example of policy Metrics Group Filters Configurations","title":"Netprobe [BETA]"},{"location":"documentation/install/","text":"Orb consists of two major components: The Control Plane \u2014comprised of microservices, communication systems, databases, etc.\u2014deploys to a central location (usually a cloud environment on Kubernetes). The Orb Agent \u2014a lightweight observability agent\u2014deploys to all the infrastructure you wish to monitor. Info The instructions below are for installing the Control Plane . If you just need to install the Orb Agent ( orb-agent ), see these instructions instead . Self-host Start by cloning the orb project in your local environment using the following command: git clone git@github.com:orb-community/orb.git A Kubernetes environment is required for both development environments and production deployment. For development, the kind tool is preferred. For production deployments, use orb-helm . Port Requirements In order for the agent to be able to communicate with the control plane, the following outgoing ports must be permitted: Protocol Port number MQTT 8883 HTTP 443 * * port 443 is only required if you want the agent to auto-provision Follow the steps from orb/kind to set up a local k8s cluster and deploy Orb. Upon successful installation, visit our Getting Started section to keep moving forward with Orb. Warning To connect an agent to the self-hosted development control plane, disable TLS verification using the environmental variable below in your provisioning command: -e ORB_TLS_VERIFY = false See example here . Bug Is the installation/deployment not working correctly? Found a bug? Come talk to us live on Slack in the #orb channel, or file a GitHub issue here . Setting up a prometheus sink In addition to deploying Orb, you can optionally set up a Prometheus instance to act as a metrics data Sink. You may wish to do this if you do not already have an existing time series database to send Orb Policy metric output to, and would like to host your own. prometheus.yml web-config.yml docker-compose.yml global : scrape_interval : 5m scrape_timeout : 1m scrape_configs : - job_name : \"prometheus\" basic_auth : username : 'MYUSER' password : 'MYPASSWORD' You will need to generate password using htpasswd as described in the prometheus documentation basic_auth_users : admin : BASIC-AUTH-PASSWORD version : \"3\" services : prometheus : image : prom/prometheus:latest ports : - 9090:9090 volumes : - \"/storage-docker/prometheus/prometheus.yaml:/etc/prometheus/prometheus.yml\" - \"/storage-docker/prometheus/web-config.yaml:/etc/prometheus/web-config.yml\" command : - '--config.file=/etc/prometheus/prometheus.yml' - '--web.config.file=/etc/prometheus/web-config.yml' - '--enable-feature=remote-write-receiver' grafana : image : grafana/grafana:latest ports : - 3000:3000 depends_on : - prometheus","title":"Installing Orb control plane"},{"location":"documentation/install/#self-host","text":"Start by cloning the orb project in your local environment using the following command: git clone git@github.com:orb-community/orb.git A Kubernetes environment is required for both development environments and production deployment. For development, the kind tool is preferred. For production deployments, use orb-helm .","title":"Self-host"},{"location":"documentation/orb_agent_configs/","text":"Configure an Orb Agent Using Configuration File Orb Agent Configuration As you can see here configuration files are needed if you want to customize your orb agent. Follow the lines below to understand how to use them and make Orb more versatile and powerful to your needs. The configuration file is written in YAML and can be used to make Orb agent to auto-provision or to configure an already connected Orb agent. Configuration File to Orb agent to auto-provision Configuration File to Orb agents already connected orb : cloud : api : address : https://orb.live token : TOKEN config : agent_name : agent_name auto_provision : true mqtt : address : tls://agents.orb.live:8883 backends : pktvisor : api_port : 10853 binary : /usr/local/sbin/pktvisord config_file : \"/path/pktvisor.yaml\" tags : key_1 : value_1 key_2 : value_2 tls : verify : true db : file : \"/orb-agent.db\" debug : enable : true visor : # this section is used by pktvisor. Check Pktvisor configuration to more information. orb : cloud : api : address : https://orb.live config : auto_provision : false mqtt : address : tls://agents.orb.live:8883 channel_id : \"AGENT_KEY_UUID\" id : \"AGENT_UUID\" key : \"AGENT_KEY_UUID\" backends : pktvisor : api_port : 10853 binary : /usr/local/sbin/pktvisord config_file : \"/path/pktvisor.yaml\" tags : key_1 : value_1 key_2 : value_2 tls : verify : true db : file : \"/orb-agent.db\" debug : enable : true visor : # this section is used by pktvisor. Check Pktvisor configuration to more information. Cloud Cloud section has three subsections: api , config and mqtt and it establishes all the necessary configurations for the agent connection. Subsection Configurations Type Required Default Extra api address str \u274c https://orb.live - api token str It's required if you want the agent to auto provision None Check here to how to create a token config agent_name str \u274c hostname It will only be used if auto_provision: True config auto_provision Bool \u274c True - mqtt address str \u274c tls://agents.orb.live:8883 - mqtt id UUID It's required if you DON'T want the agent to auto provision None Check here to understand how to get an agent id mqtt channel_id UUID It's required if you DON'T want the agent to auto provision None Check here to understand how to get an agent id mqtt key UUID It's required if you DON'T want the agent to auto provision None Check here to understand how to get an agent id Backends It is worth emphasizing that no extra installation is necessary, since the orb-agent docker container already includes the available backends. The subsections here are related to each backend supported by Orb agents. See below for configuration options: Pktvisor: Check pktvisor configurations for more information on available options to this backend. Subsection Configurations Type Required Default Meaning pktvisor binary str \u274c /usr/local/sbin/pktvisord It specifies the path on orb container where the pktvisor binary is located pktvisor config_file str \u274c /opt/orb/agent.yaml It specifies the path in orb container where the pktvisor configuration file is located and assumes that you are using the same agent file, named agent.yaml, to configure both the agent itself and pktvisor. If your pktvisor configuration file has another name/path, you must replace it with the proper one. pktvisor api_host str \u274c localhost Host where pktvisor web server will run. pktvisor api_port int \u274c 10853 Port in which pktvisor web server will run. Tags Tags are intended to dynamically define a group of agents by matching against agent group tags and are a dict type. Warning Agent tags defined on the edge (configuration files) CAN NOT be removed/edited through API or orb-website and they are not required. TLS verification It verifies the identity of the server and must be set to False if you want to use self-hosted versions. Not Required Default: True Type: Bool Debug If True , this will cause more agent logs (debug type) to be provided. Not Required Default: False Type: Bool Pktvisor Configuration For configure pktvisor you could specify taps . It's defined under visor top level key ( check an example ). The tap section specifies what data the agent should be listening in on and the goal of Taps is to abstract away host level details such as ethernet interface or dnstap socket location so that collection policies can apply to a broad set of pktvisor agents without worrying about these details. See here for more information. Single or multiple taps can be configured in the same agent. Pktvisor Configuration Structure visor : taps : first_tap_name : input_type : type config : ... filter : ... tags : key1 : value1 key2 : value2 second_tap_name : input_type : type config : ... filter : ... tags : key1 : value1 key3 : value3 The following inputs are supported: pcap , flow , dnstap and netprobe . For each input type, specific configuration, filters and tags can be defined. Packet Capture (pcap) Example: Pktvisor PCAP Tap Configuration visor : taps : my_pcap_tap : input_type : pcap config : pcap_source : \"libpcap\" debug : true iface : auto host_spec : \"192.168.0.1/24\" filter : bpf : \"port 53\" tags : pcap : true Configurations There are 5 configurations for pcap input: pcap_file , pcap_source , iface , host_spec and debug . Config Type pcap_file str pcap_source str iface str host_spec str debug bool tcp_packet_reassembly_cache_limit int pcap_file : str Back to pcap configurations list One option of using pktvisor is for reading existing network data files. In this case, the path to the file must be passed. This variable is dominant, so if a file is passed, pktvisor will do the entire process based on the file. YAML pcap_file : \"path/to/file\" pcap_source : str Back to pcap configurations list pcap_source specifies the type of library to use. Default: libpcap. Options: libpcap or af_packet (linux). YAML pcap_source : \"af_packet\" iface : str Back to pcap configurations list Name of the interface to bind. YAML iface : str Example: iface : eth0 Tip You can use auto as iface. In this way, the network with the highest data flow will be analyzed. YAML iface : auto host_spec : str Back to pcap configurations list The host_spec setting is useful to determine the direction of observed packets, once knowing the host ip, it is possible to determine the data flow direction, ie if they are being sent by the observed host (from host) or received (to host). YAML host_spec : str Example: host_spec : \"192.168.0.1/24\" debug : bool Back to pcap configurations list When true activate debug logs YAML debug : true tcp_packet_reassembly_cache_limit : int Back to pcap configurations list Sets the limit of cached packets to be reassembled. Default value: 300000 . To remove limit set tcp_packet_reassembly_cache_limit to 0 . YAML tcp_packet_reassembly_cache_limit : 300000 Filters Filter Type bpf str bpf: str Back to pcap filters list bpf filter data based on Berkeley Packet Filters (BPF). YAML bpf : str Example: bpf : \"port 53\" sFlow/Netflow (flow) Example: Pktvisor FLOW Tap Configuration visor : taps : my_flow_tap : input_type : flow config : port : 6343 bind : 192.168.1.1 flow_type : sflow tags : flow : true Configurations There are 3 configs for flow inputs: port , bind and flow_type . Config Type port int bind str flow_type str port : int and bind : str Back to sFlow/Netflow filters list The other option for using flow is specifying a port AND an ip to bind (only udp bind is supported). Note that, in this case, both variables must be set. YAML port : int bind : str Example: port : 6343 bind : 192.168.1.1 flow_type : str Back to sFlow/Netflow filters list Default: sflow. options: sflow or netflow (ipfix is supported on netflow). YAML flow_type : str Example: flow_type : netflow Filters There are no specific filters for the FLOW input. dnstap Example: Pktvisor DNSTAP Tap Configuration visor : taps : my_dnstap_tap : input_type : dnstap config : socket : path/to/file.sock tcp : 192.168.8.2:235 filter : only_hosts : 192.168.1.4/32 tags : dnstap : true Configurations The 3 existing DNSTAP configurations ( dnstap_file , socket and tcp ) are mutually exclusive, that is, only one can be used in each input and one of them must exist. They are arranged in order of priority. Config Type dnstap_file str socket int tcp str dnstap_file : str Back to dnstap configurations list One option of using pktvisor is for reading existing network data files. In this case, the path to the file must be passed. This variable is dominant, so if a file is passed, pktvisor will do the entire process based on the file. YAML dnstap_file : path/to/file socket : str Back to dnstap configurations list Path to socket file containing port and ip to bind YAML socket : path/to/file.sock tcp : str Back to dnstap configurations list The other way to inform the ip and port to be monitored is through the 'tcp' configuration. Usage syntax is a string with port:ip (only ipv4 is supported for now). YAML tcp : ip:port Example: tcp : 192.168.8.2:235 Filters Filter Type only_hosts str only_hosts : str Back to dnstap filters list only_hosts filters data from a specific host. YAML only_hosts : str Example: only_hosts : 192.168.1.4/32 Netprobe Example: Pktvisor Netprobe Tap Configuration visor : taps : default_netprobe : input_type : netprobe config : test_type : ping interval_msec : 2000 timeout_msec : 1000 packets_per_test : 10 packets_interval_msec : 25 packet_payload_size : 56 tags : netprobe : true Configurations The following configs are available for netprobe inputs: Config Type Required Default test_type str \u2705 - interval_msec int \u274c 5000 timeout_msec int \u274c 2000 packets_per_test int \u274c 1 packets_interval_msec int \u274c 25 packet_payload_size int \u274c 48 port int Required if test_type=tcp - test_type : str Back to netprobe configurations list Defines the type of the test to be performed. Type options are listed below: ping: implements a ping prober that can probe multiple targets. The test will run against the targets to verify if the systems are working fine. tcp: TCP probe sets up a TCP connection to the configured targets using the defined port. YAML test_type : str Example: test_type : ping interval_msec : int Back to netprobe configurations list How often to run the probe (in milliseconds). YAML interval_msec : int Example: interval_msec : 5000 timeout_msec : int Back to netprobe configurations list Probe timeout (in milliseconds). YAML timeout_msec : int Example: timeout_msec : 2000 packets_per_test : int Back to netprobe configurations list Number of packets to be sent in each test. YAML packets_per_test : int Example: packets_per_test : 1 packets_interval_msec : int Back to netprobe configurations list Time interval between packets per test (in milliseconds). YAML packets_interval_msec : int Example: packets_interval_msec : 25 packet_payload_size : int Back to netprobe configurations list Defines the payload of the packets sent in the tests. YAML packet_payload_size : int Example: packet_payload_size : 48 port : int Back to netprobe configurations list Specifies the port on which the TCP test will run (It is only used if the test_type is TCP. Otherwise, is ignored if set). YAML port : int Example: port : 80 Filters There are no specific filters for Netprobe input.","title":"Configuring Orb Agent"},{"location":"documentation/orb_agent_configs/#configure-an-orb-agent-using-configuration-file","text":"","title":"Configure an Orb Agent Using Configuration File"},{"location":"documentation/orb_agent_configs/#orb-agent-configuration","text":"As you can see here configuration files are needed if you want to customize your orb agent. Follow the lines below to understand how to use them and make Orb more versatile and powerful to your needs. The configuration file is written in YAML and can be used to make Orb agent to auto-provision or to configure an already connected Orb agent. Configuration File to Orb agent to auto-provision Configuration File to Orb agents already connected orb : cloud : api : address : https://orb.live token : TOKEN config : agent_name : agent_name auto_provision : true mqtt : address : tls://agents.orb.live:8883 backends : pktvisor : api_port : 10853 binary : /usr/local/sbin/pktvisord config_file : \"/path/pktvisor.yaml\" tags : key_1 : value_1 key_2 : value_2 tls : verify : true db : file : \"/orb-agent.db\" debug : enable : true visor : # this section is used by pktvisor. Check Pktvisor configuration to more information. orb : cloud : api : address : https://orb.live config : auto_provision : false mqtt : address : tls://agents.orb.live:8883 channel_id : \"AGENT_KEY_UUID\" id : \"AGENT_UUID\" key : \"AGENT_KEY_UUID\" backends : pktvisor : api_port : 10853 binary : /usr/local/sbin/pktvisord config_file : \"/path/pktvisor.yaml\" tags : key_1 : value_1 key_2 : value_2 tls : verify : true db : file : \"/orb-agent.db\" debug : enable : true visor : # this section is used by pktvisor. Check Pktvisor configuration to more information.","title":"Orb Agent Configuration"},{"location":"documentation/orb_agent_configs/#cloud","text":"Cloud section has three subsections: api , config and mqtt and it establishes all the necessary configurations for the agent connection. Subsection Configurations Type Required Default Extra api address str \u274c https://orb.live - api token str It's required if you want the agent to auto provision None Check here to how to create a token config agent_name str \u274c hostname It will only be used if auto_provision: True config auto_provision Bool \u274c True - mqtt address str \u274c tls://agents.orb.live:8883 - mqtt id UUID It's required if you DON'T want the agent to auto provision None Check here to understand how to get an agent id mqtt channel_id UUID It's required if you DON'T want the agent to auto provision None Check here to understand how to get an agent id mqtt key UUID It's required if you DON'T want the agent to auto provision None Check here to understand how to get an agent id","title":"Cloud"},{"location":"documentation/orb_agent_configs/#backends","text":"It is worth emphasizing that no extra installation is necessary, since the orb-agent docker container already includes the available backends. The subsections here are related to each backend supported by Orb agents. See below for configuration options: Pktvisor: Check pktvisor configurations for more information on available options to this backend. Subsection Configurations Type Required Default Meaning pktvisor binary str \u274c /usr/local/sbin/pktvisord It specifies the path on orb container where the pktvisor binary is located pktvisor config_file str \u274c /opt/orb/agent.yaml It specifies the path in orb container where the pktvisor configuration file is located and assumes that you are using the same agent file, named agent.yaml, to configure both the agent itself and pktvisor. If your pktvisor configuration file has another name/path, you must replace it with the proper one. pktvisor api_host str \u274c localhost Host where pktvisor web server will run. pktvisor api_port int \u274c 10853 Port in which pktvisor web server will run.","title":"Backends"},{"location":"documentation/orb_agent_configs/#tags","text":"Tags are intended to dynamically define a group of agents by matching against agent group tags and are a dict type. Warning Agent tags defined on the edge (configuration files) CAN NOT be removed/edited through API or orb-website and they are not required.","title":"Tags"},{"location":"documentation/orb_agent_configs/#tls-verification","text":"It verifies the identity of the server and must be set to False if you want to use self-hosted versions. Not Required Default: True Type: Bool","title":"TLS verification"},{"location":"documentation/orb_agent_configs/#debug","text":"If True , this will cause more agent logs (debug type) to be provided. Not Required Default: False Type: Bool","title":"Debug"},{"location":"documentation/orb_agent_configs/#pktvisor-configuration","text":"For configure pktvisor you could specify taps . It's defined under visor top level key ( check an example ). The tap section specifies what data the agent should be listening in on and the goal of Taps is to abstract away host level details such as ethernet interface or dnstap socket location so that collection policies can apply to a broad set of pktvisor agents without worrying about these details. See here for more information. Single or multiple taps can be configured in the same agent. Pktvisor Configuration Structure visor : taps : first_tap_name : input_type : type config : ... filter : ... tags : key1 : value1 key2 : value2 second_tap_name : input_type : type config : ... filter : ... tags : key1 : value1 key3 : value3 The following inputs are supported: pcap , flow , dnstap and netprobe . For each input type, specific configuration, filters and tags can be defined.","title":"Pktvisor Configuration"},{"location":"documentation/orb_agent_configs/#packet-capture-pcap","text":"Example: Pktvisor PCAP Tap Configuration visor : taps : my_pcap_tap : input_type : pcap config : pcap_source : \"libpcap\" debug : true iface : auto host_spec : \"192.168.0.1/24\" filter : bpf : \"port 53\" tags : pcap : true","title":"Packet Capture (pcap)"},{"location":"documentation/orb_agent_configs/#sflownetflow-flow","text":"Example: Pktvisor FLOW Tap Configuration visor : taps : my_flow_tap : input_type : flow config : port : 6343 bind : 192.168.1.1 flow_type : sflow tags : flow : true","title":"sFlow/Netflow (flow)"},{"location":"documentation/orb_agent_configs/#dnstap","text":"Example: Pktvisor DNSTAP Tap Configuration visor : taps : my_dnstap_tap : input_type : dnstap config : socket : path/to/file.sock tcp : 192.168.8.2:235 filter : only_hosts : 192.168.1.4/32 tags : dnstap : true","title":"dnstap"},{"location":"documentation/orb_agent_configs/#netprobe","text":"Example: Pktvisor Netprobe Tap Configuration visor : taps : default_netprobe : input_type : netprobe config : test_type : ping interval_msec : 2000 timeout_msec : 1000 packets_per_test : 10 packets_interval_msec : 25 packet_payload_size : 56 tags : netprobe : true","title":"Netprobe"},{"location":"documentation/pktvisor_metrics/","text":"Orb Metrics The Orb metrics currently provided come from the various supported pktvisor handlers and are listed here by handler. For handlers that have metric groups, the metric groups that must be enabled for the metric to exist are listed in \"Metric Groups\" column. any group* means that the metric will exist for any valid metric groups that is enabled. DHCP Metrics Check how to activate dhcp metrics Metric Prometheus Name Quantiles of all DHCP wire packets (combined ingress and egress) in packets per second dhcp_rates_total Total sum of all DHCP wire packets (combined ingress and egress) in packets per second dhcp_rates_total_sum Count of all DHCP wire packets (combined ingress and egress) in packets per second dhcp_rates_total_count Quantiles of all DHCP wire packets before filtering per second dhcp_rates_events Total sum of all DHCP wire packets before filtering per second dhcp_rates_events_sum Count of all DHCP wire packets before filtering per second dhcp_rates_events_count Top DHCP servers dhcp_top_servers Top DHCP clients dhcp_top_clients Total DHCP packets with message type ACK dhcp_wire_packets_ack Total DHCPv6 packets with message type ADVERTISE dhcp_wire_packets_advertise Total DHCP wire packets that were sampled for deep inspection dhcp_wire_packets_deep_samples Total DHCP packets with message type DISCOVER dhcp_wire_packets_discover Total DHCP wire packets events dhcp_wire_packets_events Total DHCP/DHCPv6 wire packets seen that did not match the configured filter(s) (if any) dhcp_wire_packets_filtered Total DHCP packets with message type OFFER dhcp_wire_packets_offer Total DHCPv6 packets with message type REPLY dhcp_wire_packets_reply Total DHCP packets with message type REQUEST dhcp_wire_packets_request Total DHCPv6 packets with message type REQUEST dhcp_wire_packets_request_v6 Total DHCPv6 packets with message type SOLICIT dhcp_wire_packets_solicit DORA packet counts dhcp_wire_packets_total DNS Metrics Check how to activate/deactivate dns metrics v2 v1 Warning Status: Beta . The metric names and configuration options may still change Metric Prometheus Name Metric Groups Quantiles of all DNS wire packets before filtering per second dns_rates_observed_pps any group* Count of all DNS wire packets before filtering per second dns_rates_observed_pps_count any group* Total sum of rates for DNS packets processed by policy dns_rates_observed_pps_sum any group* Total DNS transactions (query/reply pairs) with the AD flag set in the response dns_authenticated_data_xacts counters Total DNS transactions (query/reply pairs) with the AA flag set in the response dns_authoritative_answer_xacts counters Cardinality of unique QNAMES, both ingress and egress dns_cardinality_qname cardinality Total DNS transactions (query/reply pairs) with the CD flag set in the query dns_checking_disabled_xacts counters Total DNS wire packets that were sampled for deep inspection dns_deep_sampled_packets any group* Total DNS transactions (query/reply pairs) received over DNSCrypt over TCP dns_dnscrypt_tcp_xacts counters Total DNS transactions (query/reply pairs) received over DNSCrypt over UDP dns_dnscrypt_udp_xacts counters Total DNS transactions (query/reply pairs) received over DNS over HTTPS dns_doh_xacts counters Total DNS transactions (query/reply pairs) received over DNS over QUIC dns_doq_xacts counters Total DNS transactions (query/reply pairs) received over DNS over TLS dns_dot_xacts counters Total DNS transactions (query/reply pairs) with the EDNS Client Subnet option set dns_ecs_xacts counters Total DNS wire packets seen that did not match the configured filter(s) (if any) dns_filtered_packets counters Total DNS transactions (query/reply pairs) received over IPv4 dns_ipv4_xacts counters Total DNS transactions (query/reply pairs) received over IPv6 dns_ipv6_xacts counters Total DNS transactions (query/reply pairs) flagged as reply with response code NOERROR but with an empty answers section dns_nodata_xacts counters Total DNS transactions (query/reply pairs) flagged as reply with response code NOERROR dns_noerror_xacts counters Total DNS transactions (query/reply pairs) flagged as reply with response code NXDOMAIN dns_nxdomain_xacts counters Total DNS wire packets events dns_observed_packets any group* Total number of DNS responses that do not have a corresponding query dns_orphan_responses counters Total DNS transactions (query/reply pairs) flagged as reply with response code REFUSED dns_refused_xacts counters Quantiles of ratio of packet sizes in a DNS transaction (reply/query) dns_response_query_size_ratio top_size Count of ratio of packet sizes in a DNS transaction (reply/query) dns_response_query_size_ratio_count top_size Total sum of ratio of packet sizes in a DNS transaction (reply/query) dns_response_query_size_ratio_sum top_size Total DNS transactions (query/reply pairs) flagged as reply with response code SRVFAIL dns_srvfail_xacts counters Total DNS transactions (query/reply pairs) received over TCP dns_tcp_xacts counters Total number of DNS queries that timed out dns_timeout_queries counters Top ASNs by ECS dns_top_asn_ecs_xacts top_ecs Top EDNS Client Subnet (ECS) observed in DNS transaction dns_top_ecs_xacts top_ecs Top GeoIP ECS locations dns_top_geo_loc_ecs_xacts top_ecs Top QNAMES with result code NOERROR and empty answer section dns_top_nodata_xacts top_rcodes Top QNAMES with result code NOERROR dns_top_noerror_xacts top_rcodes Top QNAMES with result code NXDOMAIN dns_top_nxdomain_xacts top_rcodes Top QNAMES, aggregated at a depth of two labels dns_top_qname2_xacts top_qnames Top QNAMES, aggregated at a depth of three labels dns_top_qname3_xacts top_qnames Top QNAMES by response volume in bytes dns_top_response_bytes top_size Top query types dns_top_qtype_xacts top_qtypes Top result codes dns_top_rcode_xacts top_rcodes Top QNAMES with result code REFUSED dns_top_refused_xacts top_rcodes Top QNAMES in transactions where host is the server and transaction speed is slower than p90 dns_top_slow_xacts xact_times Top QNAMES with result code SRVFAIL dns_top_srvfail_xacts top_rcodes Top UDP source port on the query side of a transaction dns_top_udp_ports_xacts top_ports Total DNS transactions (query/reply pairs) received over UDP dns_udp_xacts counters Cumulative counters of transaction timing (query/reply pairs) in microseconds dns_xact_histogram_us_bucket xact_times Counts of transaction timing (query/reply pairs) in microseconds dns_xact_histogram_us_count xact_times Rate of all DNS transaction (reply/query) per second dns_xact_rates quantiles Count of all DNS transaction (reply/query) per second dns_xact_rates_count quantiles Total sum of all DNS transaction (reply/query) per second dns_xact_rates_sum quantiles Quantiles of transaction timing (query/reply pairs) in microseconds dns_xact_time_us xact_times Count of transaction timing (query/reply pairs) in microseconds dns_xact_time_us_count xact_times Total sum of transaction timing (query/reply pairs) in microseconds dns_xact_time_us_sum xact_times Total DNS transactions (query/reply pairs) dns_xacts counters Metric Prometheus Name Metric Groups Cardinality of unique QNAMES, both ingress and egress dns_cardinality_qname cardinality Quantiles of rates for DNS packets processed by policy dns_rates_total any group* Total sum of rates for DNS packets processed by policy dns_rates_total_sum any group* Count of rates for DNS packets processed by policy dns_rates_total_count any group* Quantiles of all DNS wire packets before filtering per second dns_rates_events any group* Total sum of all DNS wire packets before filtering per second dns_rates_events_sum any group* Count of all DNS wire packets before filtering per second dns_rates_events_count any group* Top ECS ASNs dns_top_asn_ecs top_ecs Top ECS GeoIP locations dns_top_geoLoc_ecs top_ecs Top QNAMES with response code NOERROR and no data in the response (NODATA) dns_top_nodata top_qnames Top QNAMES with response code NXDOMAIN dns_top_nxdomain top_qnames Top QNAMES by response volume dns_top_qname_by_resp_bytes top_qnames_details + top_qnames Top QNAMES with result code NOERROR dns_top_noerror top_qname_details + top_qnames Top QNAMES aggregated at a depth of two labels dns_top_qname2 top_qnames Top QNAMES aggregated at a depth of three labels dns_top_qname3 top_qnames Top QTYPEs dns_top_qtype any group* Top EDNS Client Subnets (ECS) dns_top_query_ecs top_ecs Top RCODEs dns_top_rcode any group* Top QNAMES with response code REFUSED dns_top_refused top_qnames Top QNAMES with response code SRVFAIL dns_top_srvfail top_qnames Top UDP source ports of DNS queries dns_top_udp_ports top_ports Total DNS wire packets sampled for deep inspection dns_wire_packets_deep_samples any group* Count of DNS packets sent to policy dns_wire_packets_events any group* Count of DNS packets filtered out by policy dns_wire_packets_filtered counters Count of DNS packets received over IPv4 dns_wire_packets_ipv4 counters Count of DNS packets received over IPv6 dns_wire_packets_ipv6 counters Count of DNS packets flagged as reply and identified as NODATA dns_wire_packets_nodata counters Count of DNS packets flagged as reply with response code NOERROR dns_wire_packets_noerror counters Count of DNS packets flagged as reply with response code NXDOMAIN dns_wire_packets_nxdomain counters Count of DNS packets flagged as query dns_wire_packets_queries counters Count of DNS packets with EDNS Client Subnet (ECS) option set dns_wire_packets_query_ecs counters + top_ecs Count of DNS packets flagged as reply with response code REFUSED dns_wire_packets_refused counters Count of DNS packets flagged as reply dns_wire_packets_replies counters Count of DNS packets flagged as reply with response code SRVFAIL dns_wire_packets_srvfail counters Count of DNS packets received over TCP dns_wire_packets_tcp counters Count of DNS packets matched by policy dns_wire_packets_total counters Count of DNS packets received over UDP dns_wire_packets_udp counters Total DNS wire packets received over DNS over HTTPS dns_wire_packets_doh counters (dnstap) Total DNS wire packets received over DNS over TLS dns_wire_packets_dot counters (dnstap) Total number of DNS transactions that timed out dns_xact_counts_timed_out dns_transaction Total DNS transactions (query/reply pairs) dns_xact_counts_total dns_transaction Cumulative counters for the buckets of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_histogram_us_bucket dns_transaction + histograms Count of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_histogram_us_count dns_transaction + histograms Cumulative counters for the buckets of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_histogram_us_bucket dns_transaction + histograms Count of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_histogram_us_count dns_transaction + histograms Quantiles of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_quantiles_us dns_transaction + quantiles Total sum of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_quantiles_us_sum dns_transaction + quantiles Count of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_quantiles_us_count dns_transaction + quantiles Top QNAMES in transactions where host is the server and transaction speed is slower than p90 dns_xact_in_top_slow dns_transaction Total ingress DNS transactions (host is server) dns_xact_in_total dns_transaction Quantiles of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_quantiles_us dns_transaction + quantiles Total sum of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_quantiles_us_sum dns_transaction + quantiles Count of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_quantiles_us_count dns_transaction + quantiles Top QNAMES in transactions where host is the client and transaction speed is slower than p90 dns_xact_out_top_slow dns_transaction Total egress DNS transactions (host is client) dns_xact_out_total dns_transaction Distribution of response/query size ratios dns_xact_ratio_quantiles dns_transaction + quantiles Total sum of response/query size ratios dns_xact_ratio_quantiles_sum dns_transaction + quantiles Count of response/query size ratios dns_xact_ratio_quantiles_count dns_transaction + quantiles Network Metrics Check how to activate/deactivate network metrics v2 v1 Warning Status: Beta . The metric names and configuration options may still change Metric Prometheus Name Metric Groups IP cardinality net_cardinality_ips cardinality Total packets that were sampled for deep inspection net_deep_sampled_packets any group* Total packets seen that did not match the configured filter(s) (if any) net_filtered_packets counters Count of IPv4 packets net_ipv4_packets counters Count of IPv6 packets net_ipv6_packets counters Total packets events generated net_observed_packets any group* Count of packets which are not UDP or TCP net_other_l4_packets counters Quantiles of payload sizes, in bytes net_payload_size_bytes quantiles Count of payload sizes, in bytes net_payload_size_bytes_count quantiles Total sum of payload sizes, in bytes net_payload_size_bytes_sum quantiles Data rate of bits per second net_rates_bps quantiles Count of bits per second net_rates_bps_count quantiles Total sum of bits per second net_rates_bps_sum quantiles Rate of all packets before filtering per second net_rates_observed_pps any group* Count of all packets before filtering per second net_rates_observed_pps_count any group* Total sum of all packets before filtering per second net_rates_observed_pps_sum any group* Rate of packets per second net_rates_pps quantiles Count of packets per second net_rates_pps_count quantiles Total sum of packets per second net_rates_pps_sum quantiles Count of TCP packets net_tcp_packets counters Count of TCP SYN packets net_tcp_syn_packets counters Top ASNs by IP net_top_asn_packets top_geo Top GeoIP locations net_top_geo_loc_packets top_geo Top IPv4 addresses net_top_ipv4_packets top_ips Top IPv6 addresses net_top_ipv6_packets top_ips Count of total packets matching the configured filter(s) net_total_packets counters Count of UDP packets net_udp_packets counters Metric Prometheus Name Metric Groups Destination IP cardinality packets_cardinality_dst_ips_out cardinality Source IP cardinality packets_cardinality_src_ips_in cardinality Count of packets sampled for deep inspection packets_deep_samples any group* Count of packets sent to policy packets_events any group* Count of packets filtered out by policy packets_filtered counters Count of ingress packets packets_in counters Count of IPv4 packets packets_ipv4 counters Count of IPv6 packets packets_ipv6 counters Count of packets not UDP or TCP packets_other_l4 counters Count of egress packets packets_out counters Quantiles of packet payload sizes packets_payload_size any group* Total sum of packet payload sizes packets_payload_size_sum any group* Count of packet payload sizes packets_payload_size_count any group* Count of TCP packets with SYN flag set packets_protocol_tcp_syn counters Quantiles of ingress data rates payload_rates_bytes_in any group* Total sum of ingress data rates payload_rates_bytes_in_sum any group* Count of ingress data rates payload_rates_bytes_in_count any group* Quantiles of egress data rates payload_rates_bytes_out any group* Total sum of egress data rates payload_rates_bytes_out_sum any group* Count of egress data rates payload_rates_bytes_out_count any group* Quantiles of total data rates payload_rates_bytes_total any group* Total sum of total data rates payload_rates_bytes_total_sum any group* Count of total data rates payload_rates_bytes_total_count any group* Quantiles of all packets before filtering in packets per second payload_rates_pps_events any group* Total sum of all packets before filtering in packets per second payload_rates_pps_events_sum any group* Count of all packets before filtering in packets per second payload_rates_pps_events_count any group* Quantiles of ingress packet rates payload_rates_pps_in any group* Total sum of ingress packet rates payload_rates_pps_in_sum any group* Count of ingress packet rates payload_rates_pps_in_count any group* Quantiles of egress packet rates payload_rates_pps_out any group* Total sum of egress packet rates payload_rates_pps_out_sum any group* Count of egress packet rates payload_rates_pps_out_count any group* Quantiles of total packet rates payload_rates_pps_total any group* Total sum of total packet rates payload_rates_pps_total_sum any group* Count of total packet rates payload_rates_pps_total_count any group* Count of TCP packets packets_tcp counters Top ASNs packets_top_ASN top_geo Top GeoIP locations packets_top_geoLoc top_geo Top IPv4 IP addresses packets_top_ipv4 top_ips Top IPv6 IP addresses packets_top_ipv6 top_ips Count of packets matched by policy packets_total counters Count of UDP packets packets_udp counters Count of packets of unknown direction packets_unknown_dir counters PCAP Metrics Check how to activate pcap metrics Metric Prometheus Name Total wire packets dropped by the interface pcap_if_drops Total wire packets dropped by the OS pcap_os_drops Total TCP wire packets that failed reassembly pcap_tcp_reassembly_errors BGP metrics Check how to activate bgp metrics Metric Prometheus Name Quantiles of all BGP wire packets before filtering per second bgp_rates_events Count of all BGP wire packets before filtering per second bgp_rates_events_count Total sum of all BGP wire packets before filtering per second bgp_rates_events_sum Quantiles of all BGP wire packets (combined ingress and egress) in packets per second bgp_rates_total Count of all BGP wire packets (combined ingress and egress) in packets per second bgp_rates_total_count Total sum of all BGP wire packets (combined ingress and egress) in packets per second bgp_rates_total_sum Total BGP wire packets that were sampled for deep inspection bgp_wire_packets_deep_samples Total BGP wire packets events bgp_wire_packets_events Total BGP wire packets seen that did not match the configured filter(s) (if any) bgp_wire_packets_filtered Total BGP packets with message type KEEPALIVE bgp_wire_packets_keepalive Total BGP packets with message type NOTIFICATION bgp_wire_packets_notification Total BGP packets with message type UPDATE bgp_wire_packets_update Total BGP packets with message type OPEN bgp_wire_packets_open Total BGP packets with message type ROUTEREFRESH bgp_wire_packets_routerefresh Total BGP wire packets matching the configured filter(s) bgp_wire_packets_total Flow metrics [BETA] Check how to activate/deactivate flow metrics Warning Status: Beta . The metric names and configuration options may still change Metric Prometheus Name Metric Groups Conversations cardinality flow_cardinality_conversations cardinality + conversations Destination IP cardinality flow_cardinality_dst_ips_out cardinality Destination ports cardinality flow_cardinality_dst_ports_out cardinality Source IP cardinality flow_cardinality_src_ips_in cardinality Source ports cardinality flow_cardinality_src_ports_in cardinality Count of in by bytes flow_in_bytes counters + by_bytes Count of in IPv4 by bytes flow_in_ipv4_bytes counters + by_bytes Count of in IPv4 by packets flow_in_ipv4_packets counters + by_packets Count of in IPv6 by bytes flow_in_ipv6_bytes counters + by_bytes Count of in IPv6 by packets flow_in_ipv6_packets counters + by_packets Count of in by bytes which are not UDP or TCP flow_in_other_l4_bytes counters + by_bytes Count of in by packtes which are not UDP or TCP flow_in_other_l4_packets counters + by_packets Count of in by packets flow_in_packets counters + by_packets Count of in TCP by bytes flow_in_tcp_bytes counters + by_bytes Count of in TCP by packets flow_in_tcp_packets counters + by_packets Count of in UDP by bytes flow_in_udp_bytes counters + by_bytes Count of in UDP by packets flow_in_udp_packets counters + by_packets Count of out by bytes flow_out_bytes counters + by_bytes Count of out IPV4 by bytes flow_out_ipv4_bytes counters + by_bytes Count of out IPV4 by packets flow_out_ipv4_packets counters + by_packets Count of out IPV6 by bytes flow_out_ipv6_bytes counters + by_bytes Count of out IPV6 by packets flow_out_ipv6_packets counters + by_packets Count of out by bytes which are not UDP or TCP flow_out_other_l4_bytes counters + by_bytes Count of out by packets which are not UDP or TCP flow_out_other_l4_packets counters + by_packets Count of out by packets flow_out_packets counters + by_packets Count of out TCP by bytes flow_out_tcp_bytes counters + by_bytes Count of out TCP by packets flow_out_tcp_packets counters + by_packets Count of out UDP by bytes flow_out_udp_bytes counters + by_bytes Count of out UDP by packets flow_out_udp_packets counters + by_packets Count of total flows records seen that did not match the configured filter(s) (if any) flow_records_filtered counters Count of total flows records that match the configured filter(s) (if any) flow_records_flows counters Top ASNs by IP by bytes flow_top_asn_bytes top_geo + by_bytes Top ASNs by IP by packets flow_top_asn_packets top_geo + by_packets Top source IP addresses and port by bytes flow_top_conversations_bytes conversations + by_bytes Top source IP addresses and port by packets flow_top_conversations_packets conversations + by_packets Top GeoIP locations by bytes flow_top_geo_loc_bytes top_geo + by_bytes Top GeoIP locations by packets flow_top_geo_loc_packets top_geo + by_packets Top in destination IP addresses and port by bytes flow_top_in_dst_ip_ports_bytes top_ips_ports + by_bytes Top in destination IP addresses and port by packets flow_top_in_dst_ip_ports_packets top_ips_ports + by_packets Top in destination IP addresses by bytes flow_top_in_dst_ips_bytes top_ips + by_bytes Top in destination IP addresses by packets flow_top_in_dst_ips_packets top_ips + by_packets Top in destination ports by bytes flow_top_in_dst_ports_bytes top_ports + by_bytes Top in destination ports by packets flow_top_in_dst_ports_packets top_ports + by_packets Top input interfaces by bytes flow_top_in_interfaces_bytes top_interfaces + by_bytes Top input interfaces by packets flow_top_in_interfaces_packets top_interfaces + by_packets Top in source IP addresses and port by bytes flow_top_in_src_ip_ports_bytes top_ips_ports + by_bytes Top in source IP addresses and port by packets flow_top_in_src_ip_ports_packets top_ips_ports + by_packets Top in source IP addresses by bytes flow_top_in_src_ips_bytes top_ips + by_bytes Top in source IP addresses by packets flow_top_in_src_ips_packets top_ips + by_packets Top in source ports by bytes flow_top_in_src_ports_bytes top_ports + by_bytes Top in source ports by packets flow_top_in_src_ports_packets top_ports + by_packets Top out destination IP addresses and port by bytes flow_top_out_dst_ip_ports_bytes top_ips_ports + by_bytes Top out destination IP addresses and port by packets flow_top_out_dst_ip_ports_packets top_ips_ports + by_packets Top out destination IP addresses by bytes flow_top_out_dst_ips_bytes top_ips + by_bytes Top out destination IP addresses by packets flow_top_out_dst_ips_packets top_ips + by_packets Top out destination ports by bytes flow_top_out_dst_ports_bytes top_ports + by_bytes Top out destination ports by packets flow_top_out_dst_ports_packets top_ports + by_packets Top output interfaces by bytes flow_top_out_interfaces_bytes top_interfaces + by_bytes Top output interfaces by packets flow_top_out_interfaces_packets top_interfaces + by_packets Top out source IP addresses and port by bytes flow_top_out_src_ip_ports_bytes top_ips_ports + by_bytes Top out source IP addresses and port by packets flow_top_out_src_ip_ports_packets top_ips_ports + by_packets Top out source IP addresses by bytes flow_top_out_src_ips_bytes top_ips + by_bytes Top out source IP addresses by packets flow_top_out_src_ips_packets top_ips + by_packets Top out source ports by bytes flow_top_out_src_ports_bytes top_ports + by_bytes Top out source ports by packets flow_top_out_src_ports_packets top_ports + by_packets Top in DSCP by bytes flow_top_in_dscp_bytes top_tos + by_bytes Top out DSCP by bytes flow_top_out_dscp_bytes top_tos + by_bytes Top in ECN by bytes flow_top_in_ecn_bytes top_tos + by_bytes Top out ECN by bytes flow_top_out_ecn_bytes top_tos + by_bytes Top in DSCP by packets flow_top_in_dscp_packets top_tos + by_packets Top out DSCP by packets flow_top_out_dscp_packets top_tos + by_packets Top in ECN by packets flow_top_in_ecn_packets top_tos + by_packets Top out ECN by packets flow_top_out_ecn_packets top_tos + by_packets Netprobe Metrics [BETA] Check how to activate/deactivate netprobe metrics Warning Status: Beta . The metric names and configuration options may still change Metric Prometheus Name Metric Groups Quantiles of Net Probe quantile in microseconds netprobe_response_quantiles_us quantiles Total sum of Net Probe quantile in microseconds netprobe_response_quantiles_us_sum quantiles Count of Net Probe quantile in microseconds netprobe_response_quantiles_us_count quantiles Total Net Probe attempts netprobe_attempts counters Total Net Probe failures when performed DNS lookup netprobe_dns_lookup_failures counters Total Net Probe failures when performing a TCP socket connection netprobe_connect_failures counters Total Net Probe timeout transactions netprobe_packets_timeout counters Maximum response time measured in the reporting interval netprobe_response_max_us counters + (quantiles or histograms) Minimum response time measured in the reporting interval netprobe_response_min_us counters + (quantiles or histograms) Total Net Probe successes netprobe_successes counters Cumulative counters for the buckets of Net Probe histogram in microseconds netprobe_response_histogram_us_bucket histograms Count of events of Net Probe histogram in microseconds netprobe_response_histogram_us_count histograms","title":"Orb Metrics"},{"location":"documentation/pktvisor_metrics/#orb-metrics","text":"The Orb metrics currently provided come from the various supported pktvisor handlers and are listed here by handler. For handlers that have metric groups, the metric groups that must be enabled for the metric to exist are listed in \"Metric Groups\" column. any group* means that the metric will exist for any valid metric groups that is enabled.","title":"Orb Metrics"},{"location":"documentation/pktvisor_metrics/#dhcp-metrics","text":"Check how to activate dhcp metrics Metric Prometheus Name Quantiles of all DHCP wire packets (combined ingress and egress) in packets per second dhcp_rates_total Total sum of all DHCP wire packets (combined ingress and egress) in packets per second dhcp_rates_total_sum Count of all DHCP wire packets (combined ingress and egress) in packets per second dhcp_rates_total_count Quantiles of all DHCP wire packets before filtering per second dhcp_rates_events Total sum of all DHCP wire packets before filtering per second dhcp_rates_events_sum Count of all DHCP wire packets before filtering per second dhcp_rates_events_count Top DHCP servers dhcp_top_servers Top DHCP clients dhcp_top_clients Total DHCP packets with message type ACK dhcp_wire_packets_ack Total DHCPv6 packets with message type ADVERTISE dhcp_wire_packets_advertise Total DHCP wire packets that were sampled for deep inspection dhcp_wire_packets_deep_samples Total DHCP packets with message type DISCOVER dhcp_wire_packets_discover Total DHCP wire packets events dhcp_wire_packets_events Total DHCP/DHCPv6 wire packets seen that did not match the configured filter(s) (if any) dhcp_wire_packets_filtered Total DHCP packets with message type OFFER dhcp_wire_packets_offer Total DHCPv6 packets with message type REPLY dhcp_wire_packets_reply Total DHCP packets with message type REQUEST dhcp_wire_packets_request Total DHCPv6 packets with message type REQUEST dhcp_wire_packets_request_v6 Total DHCPv6 packets with message type SOLICIT dhcp_wire_packets_solicit DORA packet counts dhcp_wire_packets_total","title":"DHCP Metrics"},{"location":"documentation/pktvisor_metrics/#dns-metrics","text":"Check how to activate/deactivate dns metrics v2 v1 Warning Status: Beta . The metric names and configuration options may still change Metric Prometheus Name Metric Groups Quantiles of all DNS wire packets before filtering per second dns_rates_observed_pps any group* Count of all DNS wire packets before filtering per second dns_rates_observed_pps_count any group* Total sum of rates for DNS packets processed by policy dns_rates_observed_pps_sum any group* Total DNS transactions (query/reply pairs) with the AD flag set in the response dns_authenticated_data_xacts counters Total DNS transactions (query/reply pairs) with the AA flag set in the response dns_authoritative_answer_xacts counters Cardinality of unique QNAMES, both ingress and egress dns_cardinality_qname cardinality Total DNS transactions (query/reply pairs) with the CD flag set in the query dns_checking_disabled_xacts counters Total DNS wire packets that were sampled for deep inspection dns_deep_sampled_packets any group* Total DNS transactions (query/reply pairs) received over DNSCrypt over TCP dns_dnscrypt_tcp_xacts counters Total DNS transactions (query/reply pairs) received over DNSCrypt over UDP dns_dnscrypt_udp_xacts counters Total DNS transactions (query/reply pairs) received over DNS over HTTPS dns_doh_xacts counters Total DNS transactions (query/reply pairs) received over DNS over QUIC dns_doq_xacts counters Total DNS transactions (query/reply pairs) received over DNS over TLS dns_dot_xacts counters Total DNS transactions (query/reply pairs) with the EDNS Client Subnet option set dns_ecs_xacts counters Total DNS wire packets seen that did not match the configured filter(s) (if any) dns_filtered_packets counters Total DNS transactions (query/reply pairs) received over IPv4 dns_ipv4_xacts counters Total DNS transactions (query/reply pairs) received over IPv6 dns_ipv6_xacts counters Total DNS transactions (query/reply pairs) flagged as reply with response code NOERROR but with an empty answers section dns_nodata_xacts counters Total DNS transactions (query/reply pairs) flagged as reply with response code NOERROR dns_noerror_xacts counters Total DNS transactions (query/reply pairs) flagged as reply with response code NXDOMAIN dns_nxdomain_xacts counters Total DNS wire packets events dns_observed_packets any group* Total number of DNS responses that do not have a corresponding query dns_orphan_responses counters Total DNS transactions (query/reply pairs) flagged as reply with response code REFUSED dns_refused_xacts counters Quantiles of ratio of packet sizes in a DNS transaction (reply/query) dns_response_query_size_ratio top_size Count of ratio of packet sizes in a DNS transaction (reply/query) dns_response_query_size_ratio_count top_size Total sum of ratio of packet sizes in a DNS transaction (reply/query) dns_response_query_size_ratio_sum top_size Total DNS transactions (query/reply pairs) flagged as reply with response code SRVFAIL dns_srvfail_xacts counters Total DNS transactions (query/reply pairs) received over TCP dns_tcp_xacts counters Total number of DNS queries that timed out dns_timeout_queries counters Top ASNs by ECS dns_top_asn_ecs_xacts top_ecs Top EDNS Client Subnet (ECS) observed in DNS transaction dns_top_ecs_xacts top_ecs Top GeoIP ECS locations dns_top_geo_loc_ecs_xacts top_ecs Top QNAMES with result code NOERROR and empty answer section dns_top_nodata_xacts top_rcodes Top QNAMES with result code NOERROR dns_top_noerror_xacts top_rcodes Top QNAMES with result code NXDOMAIN dns_top_nxdomain_xacts top_rcodes Top QNAMES, aggregated at a depth of two labels dns_top_qname2_xacts top_qnames Top QNAMES, aggregated at a depth of three labels dns_top_qname3_xacts top_qnames Top QNAMES by response volume in bytes dns_top_response_bytes top_size Top query types dns_top_qtype_xacts top_qtypes Top result codes dns_top_rcode_xacts top_rcodes Top QNAMES with result code REFUSED dns_top_refused_xacts top_rcodes Top QNAMES in transactions where host is the server and transaction speed is slower than p90 dns_top_slow_xacts xact_times Top QNAMES with result code SRVFAIL dns_top_srvfail_xacts top_rcodes Top UDP source port on the query side of a transaction dns_top_udp_ports_xacts top_ports Total DNS transactions (query/reply pairs) received over UDP dns_udp_xacts counters Cumulative counters of transaction timing (query/reply pairs) in microseconds dns_xact_histogram_us_bucket xact_times Counts of transaction timing (query/reply pairs) in microseconds dns_xact_histogram_us_count xact_times Rate of all DNS transaction (reply/query) per second dns_xact_rates quantiles Count of all DNS transaction (reply/query) per second dns_xact_rates_count quantiles Total sum of all DNS transaction (reply/query) per second dns_xact_rates_sum quantiles Quantiles of transaction timing (query/reply pairs) in microseconds dns_xact_time_us xact_times Count of transaction timing (query/reply pairs) in microseconds dns_xact_time_us_count xact_times Total sum of transaction timing (query/reply pairs) in microseconds dns_xact_time_us_sum xact_times Total DNS transactions (query/reply pairs) dns_xacts counters Metric Prometheus Name Metric Groups Cardinality of unique QNAMES, both ingress and egress dns_cardinality_qname cardinality Quantiles of rates for DNS packets processed by policy dns_rates_total any group* Total sum of rates for DNS packets processed by policy dns_rates_total_sum any group* Count of rates for DNS packets processed by policy dns_rates_total_count any group* Quantiles of all DNS wire packets before filtering per second dns_rates_events any group* Total sum of all DNS wire packets before filtering per second dns_rates_events_sum any group* Count of all DNS wire packets before filtering per second dns_rates_events_count any group* Top ECS ASNs dns_top_asn_ecs top_ecs Top ECS GeoIP locations dns_top_geoLoc_ecs top_ecs Top QNAMES with response code NOERROR and no data in the response (NODATA) dns_top_nodata top_qnames Top QNAMES with response code NXDOMAIN dns_top_nxdomain top_qnames Top QNAMES by response volume dns_top_qname_by_resp_bytes top_qnames_details + top_qnames Top QNAMES with result code NOERROR dns_top_noerror top_qname_details + top_qnames Top QNAMES aggregated at a depth of two labels dns_top_qname2 top_qnames Top QNAMES aggregated at a depth of three labels dns_top_qname3 top_qnames Top QTYPEs dns_top_qtype any group* Top EDNS Client Subnets (ECS) dns_top_query_ecs top_ecs Top RCODEs dns_top_rcode any group* Top QNAMES with response code REFUSED dns_top_refused top_qnames Top QNAMES with response code SRVFAIL dns_top_srvfail top_qnames Top UDP source ports of DNS queries dns_top_udp_ports top_ports Total DNS wire packets sampled for deep inspection dns_wire_packets_deep_samples any group* Count of DNS packets sent to policy dns_wire_packets_events any group* Count of DNS packets filtered out by policy dns_wire_packets_filtered counters Count of DNS packets received over IPv4 dns_wire_packets_ipv4 counters Count of DNS packets received over IPv6 dns_wire_packets_ipv6 counters Count of DNS packets flagged as reply and identified as NODATA dns_wire_packets_nodata counters Count of DNS packets flagged as reply with response code NOERROR dns_wire_packets_noerror counters Count of DNS packets flagged as reply with response code NXDOMAIN dns_wire_packets_nxdomain counters Count of DNS packets flagged as query dns_wire_packets_queries counters Count of DNS packets with EDNS Client Subnet (ECS) option set dns_wire_packets_query_ecs counters + top_ecs Count of DNS packets flagged as reply with response code REFUSED dns_wire_packets_refused counters Count of DNS packets flagged as reply dns_wire_packets_replies counters Count of DNS packets flagged as reply with response code SRVFAIL dns_wire_packets_srvfail counters Count of DNS packets received over TCP dns_wire_packets_tcp counters Count of DNS packets matched by policy dns_wire_packets_total counters Count of DNS packets received over UDP dns_wire_packets_udp counters Total DNS wire packets received over DNS over HTTPS dns_wire_packets_doh counters (dnstap) Total DNS wire packets received over DNS over TLS dns_wire_packets_dot counters (dnstap) Total number of DNS transactions that timed out dns_xact_counts_timed_out dns_transaction Total DNS transactions (query/reply pairs) dns_xact_counts_total dns_transaction Cumulative counters for the buckets of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_histogram_us_bucket dns_transaction + histograms Count of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_histogram_us_count dns_transaction + histograms Cumulative counters for the buckets of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_histogram_us_bucket dns_transaction + histograms Count of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_histogram_us_count dns_transaction + histograms Quantiles of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_quantiles_us dns_transaction + quantiles Total sum of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_quantiles_us_sum dns_transaction + quantiles Count of transaction timing (query/reply pairs) when host is server, in microseconds dns_xact_in_quantiles_us_count dns_transaction + quantiles Top QNAMES in transactions where host is the server and transaction speed is slower than p90 dns_xact_in_top_slow dns_transaction Total ingress DNS transactions (host is server) dns_xact_in_total dns_transaction Quantiles of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_quantiles_us dns_transaction + quantiles Total sum of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_quantiles_us_sum dns_transaction + quantiles Count of transaction timing (query/reply pairs) when host is client, in microseconds dns_xact_out_quantiles_us_count dns_transaction + quantiles Top QNAMES in transactions where host is the client and transaction speed is slower than p90 dns_xact_out_top_slow dns_transaction Total egress DNS transactions (host is client) dns_xact_out_total dns_transaction Distribution of response/query size ratios dns_xact_ratio_quantiles dns_transaction + quantiles Total sum of response/query size ratios dns_xact_ratio_quantiles_sum dns_transaction + quantiles Count of response/query size ratios dns_xact_ratio_quantiles_count dns_transaction + quantiles","title":"DNS Metrics"},{"location":"documentation/pktvisor_metrics/#network-metrics","text":"Check how to activate/deactivate network metrics v2 v1 Warning Status: Beta . The metric names and configuration options may still change Metric Prometheus Name Metric Groups IP cardinality net_cardinality_ips cardinality Total packets that were sampled for deep inspection net_deep_sampled_packets any group* Total packets seen that did not match the configured filter(s) (if any) net_filtered_packets counters Count of IPv4 packets net_ipv4_packets counters Count of IPv6 packets net_ipv6_packets counters Total packets events generated net_observed_packets any group* Count of packets which are not UDP or TCP net_other_l4_packets counters Quantiles of payload sizes, in bytes net_payload_size_bytes quantiles Count of payload sizes, in bytes net_payload_size_bytes_count quantiles Total sum of payload sizes, in bytes net_payload_size_bytes_sum quantiles Data rate of bits per second net_rates_bps quantiles Count of bits per second net_rates_bps_count quantiles Total sum of bits per second net_rates_bps_sum quantiles Rate of all packets before filtering per second net_rates_observed_pps any group* Count of all packets before filtering per second net_rates_observed_pps_count any group* Total sum of all packets before filtering per second net_rates_observed_pps_sum any group* Rate of packets per second net_rates_pps quantiles Count of packets per second net_rates_pps_count quantiles Total sum of packets per second net_rates_pps_sum quantiles Count of TCP packets net_tcp_packets counters Count of TCP SYN packets net_tcp_syn_packets counters Top ASNs by IP net_top_asn_packets top_geo Top GeoIP locations net_top_geo_loc_packets top_geo Top IPv4 addresses net_top_ipv4_packets top_ips Top IPv6 addresses net_top_ipv6_packets top_ips Count of total packets matching the configured filter(s) net_total_packets counters Count of UDP packets net_udp_packets counters Metric Prometheus Name Metric Groups Destination IP cardinality packets_cardinality_dst_ips_out cardinality Source IP cardinality packets_cardinality_src_ips_in cardinality Count of packets sampled for deep inspection packets_deep_samples any group* Count of packets sent to policy packets_events any group* Count of packets filtered out by policy packets_filtered counters Count of ingress packets packets_in counters Count of IPv4 packets packets_ipv4 counters Count of IPv6 packets packets_ipv6 counters Count of packets not UDP or TCP packets_other_l4 counters Count of egress packets packets_out counters Quantiles of packet payload sizes packets_payload_size any group* Total sum of packet payload sizes packets_payload_size_sum any group* Count of packet payload sizes packets_payload_size_count any group* Count of TCP packets with SYN flag set packets_protocol_tcp_syn counters Quantiles of ingress data rates payload_rates_bytes_in any group* Total sum of ingress data rates payload_rates_bytes_in_sum any group* Count of ingress data rates payload_rates_bytes_in_count any group* Quantiles of egress data rates payload_rates_bytes_out any group* Total sum of egress data rates payload_rates_bytes_out_sum any group* Count of egress data rates payload_rates_bytes_out_count any group* Quantiles of total data rates payload_rates_bytes_total any group* Total sum of total data rates payload_rates_bytes_total_sum any group* Count of total data rates payload_rates_bytes_total_count any group* Quantiles of all packets before filtering in packets per second payload_rates_pps_events any group* Total sum of all packets before filtering in packets per second payload_rates_pps_events_sum any group* Count of all packets before filtering in packets per second payload_rates_pps_events_count any group* Quantiles of ingress packet rates payload_rates_pps_in any group* Total sum of ingress packet rates payload_rates_pps_in_sum any group* Count of ingress packet rates payload_rates_pps_in_count any group* Quantiles of egress packet rates payload_rates_pps_out any group* Total sum of egress packet rates payload_rates_pps_out_sum any group* Count of egress packet rates payload_rates_pps_out_count any group* Quantiles of total packet rates payload_rates_pps_total any group* Total sum of total packet rates payload_rates_pps_total_sum any group* Count of total packet rates payload_rates_pps_total_count any group* Count of TCP packets packets_tcp counters Top ASNs packets_top_ASN top_geo Top GeoIP locations packets_top_geoLoc top_geo Top IPv4 IP addresses packets_top_ipv4 top_ips Top IPv6 IP addresses packets_top_ipv6 top_ips Count of packets matched by policy packets_total counters Count of UDP packets packets_udp counters Count of packets of unknown direction packets_unknown_dir counters","title":"Network Metrics"},{"location":"documentation/pktvisor_metrics/#pcap-metrics","text":"Check how to activate pcap metrics Metric Prometheus Name Total wire packets dropped by the interface pcap_if_drops Total wire packets dropped by the OS pcap_os_drops Total TCP wire packets that failed reassembly pcap_tcp_reassembly_errors","title":"PCAP Metrics"},{"location":"documentation/pktvisor_metrics/#bgp-metrics","text":"Check how to activate bgp metrics Metric Prometheus Name Quantiles of all BGP wire packets before filtering per second bgp_rates_events Count of all BGP wire packets before filtering per second bgp_rates_events_count Total sum of all BGP wire packets before filtering per second bgp_rates_events_sum Quantiles of all BGP wire packets (combined ingress and egress) in packets per second bgp_rates_total Count of all BGP wire packets (combined ingress and egress) in packets per second bgp_rates_total_count Total sum of all BGP wire packets (combined ingress and egress) in packets per second bgp_rates_total_sum Total BGP wire packets that were sampled for deep inspection bgp_wire_packets_deep_samples Total BGP wire packets events bgp_wire_packets_events Total BGP wire packets seen that did not match the configured filter(s) (if any) bgp_wire_packets_filtered Total BGP packets with message type KEEPALIVE bgp_wire_packets_keepalive Total BGP packets with message type NOTIFICATION bgp_wire_packets_notification Total BGP packets with message type UPDATE bgp_wire_packets_update Total BGP packets with message type OPEN bgp_wire_packets_open Total BGP packets with message type ROUTEREFRESH bgp_wire_packets_routerefresh Total BGP wire packets matching the configured filter(s) bgp_wire_packets_total","title":"BGP metrics"},{"location":"documentation/pktvisor_metrics/#flow-metrics-beta","text":"Check how to activate/deactivate flow metrics Warning Status: Beta . The metric names and configuration options may still change Metric Prometheus Name Metric Groups Conversations cardinality flow_cardinality_conversations cardinality + conversations Destination IP cardinality flow_cardinality_dst_ips_out cardinality Destination ports cardinality flow_cardinality_dst_ports_out cardinality Source IP cardinality flow_cardinality_src_ips_in cardinality Source ports cardinality flow_cardinality_src_ports_in cardinality Count of in by bytes flow_in_bytes counters + by_bytes Count of in IPv4 by bytes flow_in_ipv4_bytes counters + by_bytes Count of in IPv4 by packets flow_in_ipv4_packets counters + by_packets Count of in IPv6 by bytes flow_in_ipv6_bytes counters + by_bytes Count of in IPv6 by packets flow_in_ipv6_packets counters + by_packets Count of in by bytes which are not UDP or TCP flow_in_other_l4_bytes counters + by_bytes Count of in by packtes which are not UDP or TCP flow_in_other_l4_packets counters + by_packets Count of in by packets flow_in_packets counters + by_packets Count of in TCP by bytes flow_in_tcp_bytes counters + by_bytes Count of in TCP by packets flow_in_tcp_packets counters + by_packets Count of in UDP by bytes flow_in_udp_bytes counters + by_bytes Count of in UDP by packets flow_in_udp_packets counters + by_packets Count of out by bytes flow_out_bytes counters + by_bytes Count of out IPV4 by bytes flow_out_ipv4_bytes counters + by_bytes Count of out IPV4 by packets flow_out_ipv4_packets counters + by_packets Count of out IPV6 by bytes flow_out_ipv6_bytes counters + by_bytes Count of out IPV6 by packets flow_out_ipv6_packets counters + by_packets Count of out by bytes which are not UDP or TCP flow_out_other_l4_bytes counters + by_bytes Count of out by packets which are not UDP or TCP flow_out_other_l4_packets counters + by_packets Count of out by packets flow_out_packets counters + by_packets Count of out TCP by bytes flow_out_tcp_bytes counters + by_bytes Count of out TCP by packets flow_out_tcp_packets counters + by_packets Count of out UDP by bytes flow_out_udp_bytes counters + by_bytes Count of out UDP by packets flow_out_udp_packets counters + by_packets Count of total flows records seen that did not match the configured filter(s) (if any) flow_records_filtered counters Count of total flows records that match the configured filter(s) (if any) flow_records_flows counters Top ASNs by IP by bytes flow_top_asn_bytes top_geo + by_bytes Top ASNs by IP by packets flow_top_asn_packets top_geo + by_packets Top source IP addresses and port by bytes flow_top_conversations_bytes conversations + by_bytes Top source IP addresses and port by packets flow_top_conversations_packets conversations + by_packets Top GeoIP locations by bytes flow_top_geo_loc_bytes top_geo + by_bytes Top GeoIP locations by packets flow_top_geo_loc_packets top_geo + by_packets Top in destination IP addresses and port by bytes flow_top_in_dst_ip_ports_bytes top_ips_ports + by_bytes Top in destination IP addresses and port by packets flow_top_in_dst_ip_ports_packets top_ips_ports + by_packets Top in destination IP addresses by bytes flow_top_in_dst_ips_bytes top_ips + by_bytes Top in destination IP addresses by packets flow_top_in_dst_ips_packets top_ips + by_packets Top in destination ports by bytes flow_top_in_dst_ports_bytes top_ports + by_bytes Top in destination ports by packets flow_top_in_dst_ports_packets top_ports + by_packets Top input interfaces by bytes flow_top_in_interfaces_bytes top_interfaces + by_bytes Top input interfaces by packets flow_top_in_interfaces_packets top_interfaces + by_packets Top in source IP addresses and port by bytes flow_top_in_src_ip_ports_bytes top_ips_ports + by_bytes Top in source IP addresses and port by packets flow_top_in_src_ip_ports_packets top_ips_ports + by_packets Top in source IP addresses by bytes flow_top_in_src_ips_bytes top_ips + by_bytes Top in source IP addresses by packets flow_top_in_src_ips_packets top_ips + by_packets Top in source ports by bytes flow_top_in_src_ports_bytes top_ports + by_bytes Top in source ports by packets flow_top_in_src_ports_packets top_ports + by_packets Top out destination IP addresses and port by bytes flow_top_out_dst_ip_ports_bytes top_ips_ports + by_bytes Top out destination IP addresses and port by packets flow_top_out_dst_ip_ports_packets top_ips_ports + by_packets Top out destination IP addresses by bytes flow_top_out_dst_ips_bytes top_ips + by_bytes Top out destination IP addresses by packets flow_top_out_dst_ips_packets top_ips + by_packets Top out destination ports by bytes flow_top_out_dst_ports_bytes top_ports + by_bytes Top out destination ports by packets flow_top_out_dst_ports_packets top_ports + by_packets Top output interfaces by bytes flow_top_out_interfaces_bytes top_interfaces + by_bytes Top output interfaces by packets flow_top_out_interfaces_packets top_interfaces + by_packets Top out source IP addresses and port by bytes flow_top_out_src_ip_ports_bytes top_ips_ports + by_bytes Top out source IP addresses and port by packets flow_top_out_src_ip_ports_packets top_ips_ports + by_packets Top out source IP addresses by bytes flow_top_out_src_ips_bytes top_ips + by_bytes Top out source IP addresses by packets flow_top_out_src_ips_packets top_ips + by_packets Top out source ports by bytes flow_top_out_src_ports_bytes top_ports + by_bytes Top out source ports by packets flow_top_out_src_ports_packets top_ports + by_packets Top in DSCP by bytes flow_top_in_dscp_bytes top_tos + by_bytes Top out DSCP by bytes flow_top_out_dscp_bytes top_tos + by_bytes Top in ECN by bytes flow_top_in_ecn_bytes top_tos + by_bytes Top out ECN by bytes flow_top_out_ecn_bytes top_tos + by_bytes Top in DSCP by packets flow_top_in_dscp_packets top_tos + by_packets Top out DSCP by packets flow_top_out_dscp_packets top_tos + by_packets Top in ECN by packets flow_top_in_ecn_packets top_tos + by_packets Top out ECN by packets flow_top_out_ecn_packets top_tos + by_packets","title":"Flow metrics [BETA]"},{"location":"documentation/pktvisor_metrics/#netprobe-metrics-beta","text":"Check how to activate/deactivate netprobe metrics Warning Status: Beta . The metric names and configuration options may still change Metric Prometheus Name Metric Groups Quantiles of Net Probe quantile in microseconds netprobe_response_quantiles_us quantiles Total sum of Net Probe quantile in microseconds netprobe_response_quantiles_us_sum quantiles Count of Net Probe quantile in microseconds netprobe_response_quantiles_us_count quantiles Total Net Probe attempts netprobe_attempts counters Total Net Probe failures when performed DNS lookup netprobe_dns_lookup_failures counters Total Net Probe failures when performing a TCP socket connection netprobe_connect_failures counters Total Net Probe timeout transactions netprobe_packets_timeout counters Maximum response time measured in the reporting interval netprobe_response_max_us counters + (quantiles or histograms) Minimum response time measured in the reporting interval netprobe_response_min_us counters + (quantiles or histograms) Total Net Probe successes netprobe_successes counters Cumulative counters for the buckets of Net Probe histogram in microseconds netprobe_response_histogram_us_bucket histograms Count of events of Net Probe histogram in microseconds netprobe_response_histogram_us_count histograms","title":"Netprobe Metrics [BETA]"},{"location":"documentation/running_orb_agent/","text":"Running Orb Agent An Orb agent needs to run on all the infrastructure (computers, servers, switches, VMs, k8s, etc.) to be monitored. It is a small, lightweight Docker process with an embedded pktvisor agent which connects into the Orb control plane to receive policies and send its metric output. To run an agent, you will need: Docker, to run the agent image ( orbcommunity/orb-agent:develop ) Agent Credentials , which are provided to you by the Orb UI or REST API after creating an agent The Orb Control Plane host address (e.g. localhost or orb.live ) Tip If you are unsure which network interface to monitor, you may list the available interfaces on your host. Note that to allow the agent access to these interfaces, you must run the container with --net=host Linux OSX ip -stats -color -human addr ifconfig Agent credentials The agent credentials include three pieces of information , each of which is a UUID in the form 5dc34ded-6a53-44c0-8d15-7e9c8c95391a . Agent ID , which uniquely identifies the agent. Agent Channel ID , which uniquely identifies the agent's communication channel. Agent Key , which is a private access token for the agent. Note you will only be shown the key once upon creation! Sample provisioning commands Example Generic Use this command as a template by substituting in the appropriate values: docker run -d --net = host -e ORB_CLOUD_ADDRESS = <HOST> -e ORB_CLOUD_MQTT_ID = <AGENTID> -e ORB_CLOUD_MQTT_CHANNEL_ID = <CHANNELID> -e ORB_CLOUD_MQTT_KEY = <AGENTKEY> -e PKTVISOR_PCAP_IFACE_DEFAULT = auto orbcommunity/orb-agent Localhost, Docker Compose This command is useful for connecting to a local develop environment, perhaps running on Docker compose . Note that the \"mock\" interface will generate random traffic rather than observe real traffic. docker run -d --net = host -e ORB_CLOUD_ADDRESS = localhost -e ORB_CLOUD_MQTT_ID = 7fb96f61-5de1-4f56-99d6-4eb8b43f8bad -e ORB_CLOUD_MQTT_CHANNEL_ID = 3e60e85d-4414-44d9-b564-0c1874898a4d -e ORB_CLOUD_MQTT_KEY = 44e42d90-aaef-45de-9bc2-2b2581eb30b3 -e PKTVISOR_PCAP_IFACE_DEFAULT = mock -e ORB_TLS_VERIFY = false orbcommunity/orb-agent Orb.live, eth0 This command is similar to one you would use on the orb.live SaaS platform docker run -d --net = host -e ORB_CLOUD_ADDRESS = orb.live -e ORB_CLOUD_MQTT_ID = 7fb96f61-5de1-4f56-99d6-4eb8b43f8bad -e ORB_CLOUD_MQTT_CHANNEL_ID = 3e60e85d-4414-44d9-b564-0c1874898a4d -e ORB_CLOUD_MQTT_KEY = 44e42d90-aaef-45de-9bc2-2b2581eb30b3 -e PKTVISOR_PCAP_IFACE_DEFAULT = eth0 orbcommunity/orb-agent Specifying agent port You may want to run more than one agent on the same node and for that you must specify different pktvisor control ports for them, since the containers run in host networking mode, only one is allowed to run per port. By default, the pktvisor control port runs on port 10853 , but this value can be set through the environment variable ORB_BACKENDS_PKTVISOR_API_PORT docker run -d --net = host -e ORB_CLOUD_ADDRESS = orb.live -e ORB_CLOUD_MQTT_ID = 7fb96f61-5de1-4f56-99d6-4eb8b43f8bad -e ORB_CLOUD_MQTT_CHANNEL_ID = 3e60e85d-4414-44d9-b564-0c1874898a4d -e ORB_CLOUD_MQTT_KEY = 44e42d90-aaef-45de-9bc2-2b2581eb30b3 -e PKTVISOR_PCAP_IFACE_DEFAULT = eth0 -e ORB_BACKENDS_PKTVISOR_API_PORT = 10854 orbcommunity/orb-agent \ud83c\udf81 BONUS - Debug You can access agent debug logs by passing the -d command docker run -d --net = host -e ORB_CLOUD_ADDRESS = orb.live -e ORB_CLOUD_MQTT_ID = 7fb96f61-5de1-4f56-99d6-4eb8b43f8bad -e ORB_CLOUD_MQTT_CHANNEL_ID = 3e60e85d-4414-44d9-b564-0c1874898a4d -e ORB_CLOUD_MQTT_KEY = 44e42d90-aaef-45de-9bc2-2b2581eb30b3 -e PKTVISOR_PCAP_IFACE_DEFAULT = eth0 orbcommunity/orb-agent run -d Question Is the agent Docker image not starting correctly? Do you have a specific use case? Have you found a bug? Come talk to us live on Slack in the #orb channel, or file a GitHub issue here . Configuration files Most configuration options can be passed to the container as environment variables, but there are some situations that require a configuration file. You will need to use a configuration file if: You want to assign tags to the agent at the edge You want to set up custom pktvisor Taps You want the agent to auto-provision The configuration file is written in YAML. You can use the latest template configuration file as a starting point and check more information in Configuring Orb Agent , or start here: # this section is used by orb-agent # most sections and keys are optional orb : # these are arbitrary key value pairs used for dynamically define a group of agents by matching against agent group tags tags : region : EU pop : ams02 node_type : dns cloud : config : # optionally specify an agent name to use during auto provisioning # hostname will be used if it's not specified here agent_name : my-agent1 auto_provision : true api : address : https://orb.live # if auto provisioning, specify API token here (or pass on the command line) token : TOKEN mqtt : address : tls://agents.orb.live:8883 # if not auto provisioning, specify agent connection details here id : \"AGENT_UUID\" key : \"AGENT_KEY_UUID\" channel_id : \"AGENT_CHANNEL_UUID\" backends : pktvisor : binary : \"/usr/local/sbin/pktvisord\" # this example assumes the file is saved as agent.yaml. If your file has another name, you must replace it with the proper name config_file : \"/opt/orb/agent.yaml\" version : \"1.0\" # this section is used by pktvisor # see https://orb.community/documentation/orb_agent_configs/#pktvisor-configuration visor : taps : default_pcap : input_type : pcap config : iface : \"eth0\" host_spec : \"192.168.0.54/32,192.168.0.55/32,127.0.0.1/32\" You must mount your configuration file into the orb-agent container. For example, if your configuration file is on the host at /local/orb/agent.yaml , you can mount it into the container with this command: docker run -v /local/orb:/opt/orb/ --net = host \\ orbcommunity/orb-agent run -c /opt/orb/agent.yaml Advanced auto-provisioning setup Some use cases require a way to provision agents directly on edge infrastructure without creating an agent manually in the UI or REST API ahead of time. To do so, you will need to create an API key which can be used by orb-agent to provision itself. Warning Auto-provisioning is an advanced use case. Most users will find creating an agent in the UI easier. If you have not already done so, register a new account with an email address and password at https://HOST/auth/register. Create a SESSION_TOKEN with the EMAIL_ADDRESS and PASSWORD from registration: curl --location --request POST 'https://HOST/api/v1/tokens' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"email\": \"<EMAIL_ADDRESS>\", \"password\": \"<PASSWORD>\" }' The output from creating a session token looks like this: { \"token\": \"SESSION_TOKEN\" } Because session tokens expire after 24 hours, you can create a permanent API token for agent provisioning by using the SESSION_TOKEN above: curl --location --request POST 'https://HOST/api/v1/keys' \\ --header 'Authorization: Bearer <SESSION_TOKEN>' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"type\": 2 }' The output from creating a PERMANENT_TOKEN looks like the following. Please take note of the id (used later to revoke) and the value (the permanent API token): { \"id\": \"710c6a92-b463-42ec-bf24-8ae24eb13081\", \"value\": \"PERMANENT_TOKEN\", \"issued_at\": \"2021-09-07T15:29:49.70146088Z\" } Currently, the permanent token allows access to all API functionality, not just provisioning. You can revoke this permanent token at any time with the following call, using the id field above: curl --location --request DELETE 'HOST:80/api/v1/keys/<PERMANENT_TOKEN_ID>' \\ --header 'Authorization: Bearer <SESSION_TOKEN>' Create a config for Orb and pktvisor taps, for example, /opt/orb/agent.yaml : version : \"1.0\" visor : taps : ethernet : input_type : pcap config : iface : \"eth0\" orb : db : file : /usr/local/orb/orb-agent.db tags : region : EU pop : ams02 node_type : dns cloud : config : agent_name : myagent1 api : address : https://HOST mqtt : address : tls://HOST:8883 You can now pull and run orbcommunity/orb-agent to auto-provision, substituting in the PERMANENT_TOKEN and optionally configuring agent name and Orb tags. If you don't set the agent name, it will attempt to use a hostname. You must mount the directory to save the agent state database and the config file: docker pull orbcommunity/orb-agent docker run -v /local/orb:/opt/orb/ --net = host \\ -e ORB_CLOUD_API_TOKEN = <PERMANENT_TOKEN> \\ orbcommunity/orb-agent run -c /opt/orb/agent.yaml","title":"Running Orb Agents"},{"location":"documentation/running_orb_agent/#running-orb-agent","text":"An Orb agent needs to run on all the infrastructure (computers, servers, switches, VMs, k8s, etc.) to be monitored. It is a small, lightweight Docker process with an embedded pktvisor agent which connects into the Orb control plane to receive policies and send its metric output. To run an agent, you will need: Docker, to run the agent image ( orbcommunity/orb-agent:develop ) Agent Credentials , which are provided to you by the Orb UI or REST API after creating an agent The Orb Control Plane host address (e.g. localhost or orb.live ) Tip If you are unsure which network interface to monitor, you may list the available interfaces on your host. Note that to allow the agent access to these interfaces, you must run the container with --net=host Linux OSX ip -stats -color -human addr ifconfig","title":"Running Orb Agent"},{"location":"documentation/running_orb_agent/#agent-credentials","text":"The agent credentials include three pieces of information , each of which is a UUID in the form 5dc34ded-6a53-44c0-8d15-7e9c8c95391a . Agent ID , which uniquely identifies the agent. Agent Channel ID , which uniquely identifies the agent's communication channel. Agent Key , which is a private access token for the agent. Note you will only be shown the key once upon creation!","title":"Agent credentials"},{"location":"documentation/running_orb_agent/#sample-provisioning-commands","text":"Example Generic Use this command as a template by substituting in the appropriate values: docker run -d --net = host -e ORB_CLOUD_ADDRESS = <HOST> -e ORB_CLOUD_MQTT_ID = <AGENTID> -e ORB_CLOUD_MQTT_CHANNEL_ID = <CHANNELID> -e ORB_CLOUD_MQTT_KEY = <AGENTKEY> -e PKTVISOR_PCAP_IFACE_DEFAULT = auto orbcommunity/orb-agent Localhost, Docker Compose This command is useful for connecting to a local develop environment, perhaps running on Docker compose . Note that the \"mock\" interface will generate random traffic rather than observe real traffic. docker run -d --net = host -e ORB_CLOUD_ADDRESS = localhost -e ORB_CLOUD_MQTT_ID = 7fb96f61-5de1-4f56-99d6-4eb8b43f8bad -e ORB_CLOUD_MQTT_CHANNEL_ID = 3e60e85d-4414-44d9-b564-0c1874898a4d -e ORB_CLOUD_MQTT_KEY = 44e42d90-aaef-45de-9bc2-2b2581eb30b3 -e PKTVISOR_PCAP_IFACE_DEFAULT = mock -e ORB_TLS_VERIFY = false orbcommunity/orb-agent Orb.live, eth0 This command is similar to one you would use on the orb.live SaaS platform docker run -d --net = host -e ORB_CLOUD_ADDRESS = orb.live -e ORB_CLOUD_MQTT_ID = 7fb96f61-5de1-4f56-99d6-4eb8b43f8bad -e ORB_CLOUD_MQTT_CHANNEL_ID = 3e60e85d-4414-44d9-b564-0c1874898a4d -e ORB_CLOUD_MQTT_KEY = 44e42d90-aaef-45de-9bc2-2b2581eb30b3 -e PKTVISOR_PCAP_IFACE_DEFAULT = eth0 orbcommunity/orb-agent Specifying agent port You may want to run more than one agent on the same node and for that you must specify different pktvisor control ports for them, since the containers run in host networking mode, only one is allowed to run per port. By default, the pktvisor control port runs on port 10853 , but this value can be set through the environment variable ORB_BACKENDS_PKTVISOR_API_PORT docker run -d --net = host -e ORB_CLOUD_ADDRESS = orb.live -e ORB_CLOUD_MQTT_ID = 7fb96f61-5de1-4f56-99d6-4eb8b43f8bad -e ORB_CLOUD_MQTT_CHANNEL_ID = 3e60e85d-4414-44d9-b564-0c1874898a4d -e ORB_CLOUD_MQTT_KEY = 44e42d90-aaef-45de-9bc2-2b2581eb30b3 -e PKTVISOR_PCAP_IFACE_DEFAULT = eth0 -e ORB_BACKENDS_PKTVISOR_API_PORT = 10854 orbcommunity/orb-agent \ud83c\udf81 BONUS - Debug You can access agent debug logs by passing the -d command docker run -d --net = host -e ORB_CLOUD_ADDRESS = orb.live -e ORB_CLOUD_MQTT_ID = 7fb96f61-5de1-4f56-99d6-4eb8b43f8bad -e ORB_CLOUD_MQTT_CHANNEL_ID = 3e60e85d-4414-44d9-b564-0c1874898a4d -e ORB_CLOUD_MQTT_KEY = 44e42d90-aaef-45de-9bc2-2b2581eb30b3 -e PKTVISOR_PCAP_IFACE_DEFAULT = eth0 orbcommunity/orb-agent run -d Question Is the agent Docker image not starting correctly? Do you have a specific use case? Have you found a bug? Come talk to us live on Slack in the #orb channel, or file a GitHub issue here .","title":"Sample provisioning commands"},{"location":"documentation/running_orb_agent/#configuration-files","text":"Most configuration options can be passed to the container as environment variables, but there are some situations that require a configuration file. You will need to use a configuration file if: You want to assign tags to the agent at the edge You want to set up custom pktvisor Taps You want the agent to auto-provision The configuration file is written in YAML. You can use the latest template configuration file as a starting point and check more information in Configuring Orb Agent , or start here: # this section is used by orb-agent # most sections and keys are optional orb : # these are arbitrary key value pairs used for dynamically define a group of agents by matching against agent group tags tags : region : EU pop : ams02 node_type : dns cloud : config : # optionally specify an agent name to use during auto provisioning # hostname will be used if it's not specified here agent_name : my-agent1 auto_provision : true api : address : https://orb.live # if auto provisioning, specify API token here (or pass on the command line) token : TOKEN mqtt : address : tls://agents.orb.live:8883 # if not auto provisioning, specify agent connection details here id : \"AGENT_UUID\" key : \"AGENT_KEY_UUID\" channel_id : \"AGENT_CHANNEL_UUID\" backends : pktvisor : binary : \"/usr/local/sbin/pktvisord\" # this example assumes the file is saved as agent.yaml. If your file has another name, you must replace it with the proper name config_file : \"/opt/orb/agent.yaml\" version : \"1.0\" # this section is used by pktvisor # see https://orb.community/documentation/orb_agent_configs/#pktvisor-configuration visor : taps : default_pcap : input_type : pcap config : iface : \"eth0\" host_spec : \"192.168.0.54/32,192.168.0.55/32,127.0.0.1/32\" You must mount your configuration file into the orb-agent container. For example, if your configuration file is on the host at /local/orb/agent.yaml , you can mount it into the container with this command: docker run -v /local/orb:/opt/orb/ --net = host \\ orbcommunity/orb-agent run -c /opt/orb/agent.yaml","title":"Configuration files"},{"location":"documentation/running_orb_agent/#advanced-auto-provisioning-setup","text":"Some use cases require a way to provision agents directly on edge infrastructure without creating an agent manually in the UI or REST API ahead of time. To do so, you will need to create an API key which can be used by orb-agent to provision itself. Warning Auto-provisioning is an advanced use case. Most users will find creating an agent in the UI easier. If you have not already done so, register a new account with an email address and password at https://HOST/auth/register. Create a SESSION_TOKEN with the EMAIL_ADDRESS and PASSWORD from registration: curl --location --request POST 'https://HOST/api/v1/tokens' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"email\": \"<EMAIL_ADDRESS>\", \"password\": \"<PASSWORD>\" }' The output from creating a session token looks like this: { \"token\": \"SESSION_TOKEN\" } Because session tokens expire after 24 hours, you can create a permanent API token for agent provisioning by using the SESSION_TOKEN above: curl --location --request POST 'https://HOST/api/v1/keys' \\ --header 'Authorization: Bearer <SESSION_TOKEN>' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"type\": 2 }' The output from creating a PERMANENT_TOKEN looks like the following. Please take note of the id (used later to revoke) and the value (the permanent API token): { \"id\": \"710c6a92-b463-42ec-bf24-8ae24eb13081\", \"value\": \"PERMANENT_TOKEN\", \"issued_at\": \"2021-09-07T15:29:49.70146088Z\" } Currently, the permanent token allows access to all API functionality, not just provisioning. You can revoke this permanent token at any time with the following call, using the id field above: curl --location --request DELETE 'HOST:80/api/v1/keys/<PERMANENT_TOKEN_ID>' \\ --header 'Authorization: Bearer <SESSION_TOKEN>' Create a config for Orb and pktvisor taps, for example, /opt/orb/agent.yaml : version : \"1.0\" visor : taps : ethernet : input_type : pcap config : iface : \"eth0\" orb : db : file : /usr/local/orb/orb-agent.db tags : region : EU pop : ams02 node_type : dns cloud : config : agent_name : myagent1 api : address : https://HOST mqtt : address : tls://HOST:8883 You can now pull and run orbcommunity/orb-agent to auto-provision, substituting in the PERMANENT_TOKEN and optionally configuring agent name and Orb tags. If you don't set the agent name, it will attempt to use a hostname. You must mount the directory to save the agent state database and the config file: docker pull orbcommunity/orb-agent docker run -v /local/orb:/opt/orb/ --net = host \\ -e ORB_CLOUD_API_TOKEN = <PERMANENT_TOKEN> \\ orbcommunity/orb-agent run -c /opt/orb/agent.yaml","title":"Advanced auto-provisioning setup"},{"location":"guides/","text":"Solution Guides The following solution guides provide detailed instructions and examples for configuring Orb agents to monitor some common Observability Use Cases: Observability for Edge Networks Observability for Authoritative DNS Services Observability for Recursive DNS Services Observability for DHCP Services Observability for Network Traffic (Flow) Observability for Active Response Time Testing","title":"Solution Guides"},{"location":"guides/#solution-guides","text":"The following solution guides provide detailed instructions and examples for configuring Orb agents to monitor some common Observability Use Cases: Observability for Edge Networks Observability for Authoritative DNS Services Observability for Recursive DNS Services Observability for DHCP Services Observability for Network Traffic (Flow) Observability for Active Response Time Testing","title":"Solution Guides"},{"location":"guides/authoritative_dns/","text":"This solution guide will walk you through setting up Observability for Authoritative DNS Services using Orb agents. This is a monitoring configuration that can be used when you want to monitor only inbound DNS queries and their responses. Configure Observation The first step is to configure the agents to observe only inbound DNS traffic. This can be done using either the PCAP (Packet Capture) or the DNSTAP taps. PCAP tap configuration Using PCAP (Packet Capture), your agent tap configurations should look something like this: YAML JSON taps : dns_pcap : interface : \"visor.module.input/1.0\" input_type : pcap config : iface : \"auto\" bpf : '(dst port 53 and dst net 172.16.2.0/24) or (src port 53 and src net 172.16.2.0/24)' host_spec : '172.16.2.0/24' { \"taps\" : { \"dns_pcap\" : { \"config\" : { \"iface\" : \"auto\" , \"bpf\" : \"(dst port 53 and dst net 172.16.2.0/24) or (src port 53 and src net 172.16.2.0/24)\" , \"host_spec\" : \"172.16.2.0/24\" }, \"input_type\" : \"pcap\" , \"interface\" : \"visor.module.input/1.0\" } } } If you want to explicitely specify the interface the PCAP tap should be listening on (instead of using the auto selection), please ensure that the iface is configured with the appropriate interface name. If the interface defined is not correct, you will need to re-run the agent provisioning command specifying the correct interface using the PKTVISOR_PCAP_IFACE_DEFAULT environment variable. Configure Analysis The second step is to configure the agents to analyze for DNS traffic. This is done by defining and applying a tailored Policy to the agents. Create a Policy The following is a sample policy that includes the NET and DNS handlers. Your policy should look something like this: YAML JSON handlers : modules : - handler_dns_1 : config : public_suffix_list : true metric_groups : enable : - top_ecs type : dns - handler_net_1 : type : net input : input_type : pcap tap : dns_pcap kind : collection \"handlers\" : { \"modules\" : [ { \"handler_dns_1\" : { \"config\" : { \"public_suffix_list\" : true }, \"metric_groups\" : { \"enable\" : [ \"top_ecs\" ] }, \"type\" : \"dns\" } }, { \"handler_net_1\" : { \"type\" : \"net\" } } ] }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"dns_pcap\" }, \"kind\" : \"collection\" For a more tailored DNS policy to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference . Applying your policy The final step is to apply your policy by creating a Dataset , which links your agents (through an Agent Group ) with your policy and the Sink where you want to send your metrics.","title":"Authoritative DNS"},{"location":"guides/authoritative_dns/#configure-observation","text":"The first step is to configure the agents to observe only inbound DNS traffic. This can be done using either the PCAP (Packet Capture) or the DNSTAP taps.","title":"Configure Observation"},{"location":"guides/authoritative_dns/#pcap-tap-configuration","text":"Using PCAP (Packet Capture), your agent tap configurations should look something like this: YAML JSON taps : dns_pcap : interface : \"visor.module.input/1.0\" input_type : pcap config : iface : \"auto\" bpf : '(dst port 53 and dst net 172.16.2.0/24) or (src port 53 and src net 172.16.2.0/24)' host_spec : '172.16.2.0/24' { \"taps\" : { \"dns_pcap\" : { \"config\" : { \"iface\" : \"auto\" , \"bpf\" : \"(dst port 53 and dst net 172.16.2.0/24) or (src port 53 and src net 172.16.2.0/24)\" , \"host_spec\" : \"172.16.2.0/24\" }, \"input_type\" : \"pcap\" , \"interface\" : \"visor.module.input/1.0\" } } } If you want to explicitely specify the interface the PCAP tap should be listening on (instead of using the auto selection), please ensure that the iface is configured with the appropriate interface name. If the interface defined is not correct, you will need to re-run the agent provisioning command specifying the correct interface using the PKTVISOR_PCAP_IFACE_DEFAULT environment variable.","title":"PCAP tap configuration"},{"location":"guides/authoritative_dns/#configure-analysis","text":"The second step is to configure the agents to analyze for DNS traffic. This is done by defining and applying a tailored Policy to the agents.","title":"Configure Analysis"},{"location":"guides/authoritative_dns/#create-a-policy","text":"The following is a sample policy that includes the NET and DNS handlers. Your policy should look something like this: YAML JSON handlers : modules : - handler_dns_1 : config : public_suffix_list : true metric_groups : enable : - top_ecs type : dns - handler_net_1 : type : net input : input_type : pcap tap : dns_pcap kind : collection \"handlers\" : { \"modules\" : [ { \"handler_dns_1\" : { \"config\" : { \"public_suffix_list\" : true }, \"metric_groups\" : { \"enable\" : [ \"top_ecs\" ] }, \"type\" : \"dns\" } }, { \"handler_net_1\" : { \"type\" : \"net\" } } ] }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"dns_pcap\" }, \"kind\" : \"collection\" For a more tailored DNS policy to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference .","title":"Create a Policy"},{"location":"guides/authoritative_dns/#applying-your-policy","text":"The final step is to apply your policy by creating a Dataset , which links your agents (through an Agent Group ) with your policy and the Sink where you want to send your metrics.","title":"Applying your policy"},{"location":"guides/dhcp_services/","text":"This solution guide will walk you through setting up Observability for DHCP Services using Orb agents. This is a monitoring configuration that can be used when monitoring of DHCP services is required. Configure Observation The first step is to configure what data the agents should be observing. This is done by defining and configuring what Tap the agents should be listenining on. The default PCAP (Packet Capture) tap configuration is all that is necessary for DHCP monitoring. Your agent tap configurations should look something like this: YAML JSON taps : default_pcap : interface : \"visor.module.input/1.0\" input_type : pcap config : iface : \"auto\" { \"taps\" : { \"default_pcap\" : { \"config\" : { \"iface\" : \"auto\" , }, \"input_type\" : \"pcap\" , \"interface\" : \"visor.module.input/1.0\" } } } If you want to explicitely specify the interface the PCAP tap should be listening on (instead of using the auto selection), please ensure that the iface is configured with the appropriate interface name. If the interface defined is not correct, you will need to re-run the agent provisioning command specifying the correct interface using the PKTVISOR_PCAP_IFACE_DEFAULT environment variable. Configure Analysis The second step is to configure the agents to analyze for DHCP traffic. This is done by defining and applying a tailored Policy to the agents. Create a Policy The following is a sample policy that includes the NET and DHCP handlers, as well as a BPF filter to restrict analyzed data to DHCP traffic. Your policy should look something like this: YAML JSON handlers : modules : handler_dns_1 : type : dhcp handler_net_1 : type : net input : input_type : pcap tap : default_pcap filter : bpf : 'port 67 or port 68' kind : collection \"handlers\" : { \"modules\" : { \"handler_dhcp_1\" : { \"type\" : \"dhcp\" }, \"handler_net_1\" : { \"type\" : \"net\" } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"default_pcap\" , \"filter\" : { \"bpf\" : \"port 67 or port 68\" } }, \"kind\" : \"collection\" For a more tailored DHCP policy to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference . Applying your policy The final step is to apply your policy by creating a Dataset , which links your agents (through an Agent Group ) with your policy and the Sink where you want to send your metrics.","title":"DHCP Services"},{"location":"guides/dhcp_services/#configure-observation","text":"The first step is to configure what data the agents should be observing. This is done by defining and configuring what Tap the agents should be listenining on. The default PCAP (Packet Capture) tap configuration is all that is necessary for DHCP monitoring. Your agent tap configurations should look something like this: YAML JSON taps : default_pcap : interface : \"visor.module.input/1.0\" input_type : pcap config : iface : \"auto\" { \"taps\" : { \"default_pcap\" : { \"config\" : { \"iface\" : \"auto\" , }, \"input_type\" : \"pcap\" , \"interface\" : \"visor.module.input/1.0\" } } } If you want to explicitely specify the interface the PCAP tap should be listening on (instead of using the auto selection), please ensure that the iface is configured with the appropriate interface name. If the interface defined is not correct, you will need to re-run the agent provisioning command specifying the correct interface using the PKTVISOR_PCAP_IFACE_DEFAULT environment variable.","title":"Configure Observation"},{"location":"guides/dhcp_services/#configure-analysis","text":"The second step is to configure the agents to analyze for DHCP traffic. This is done by defining and applying a tailored Policy to the agents.","title":"Configure Analysis"},{"location":"guides/dhcp_services/#create-a-policy","text":"The following is a sample policy that includes the NET and DHCP handlers, as well as a BPF filter to restrict analyzed data to DHCP traffic. Your policy should look something like this: YAML JSON handlers : modules : handler_dns_1 : type : dhcp handler_net_1 : type : net input : input_type : pcap tap : default_pcap filter : bpf : 'port 67 or port 68' kind : collection \"handlers\" : { \"modules\" : { \"handler_dhcp_1\" : { \"type\" : \"dhcp\" }, \"handler_net_1\" : { \"type\" : \"net\" } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"default_pcap\" , \"filter\" : { \"bpf\" : \"port 67 or port 68\" } }, \"kind\" : \"collection\" For a more tailored DHCP policy to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference .","title":"Create a Policy"},{"location":"guides/dhcp_services/#applying-your-policy","text":"The final step is to apply your policy by creating a Dataset , which links your agents (through an Agent Group ) with your policy and the Sink where you want to send your metrics.","title":"Applying your policy"},{"location":"guides/edge_networks/","text":"This solution guide will walk you through setting up Observability for Edge Networks using Orb agents. This is a general purpose monitoring configuration that can be used wherever general observability of the network is required. Configure Observation The first step is to configure what data the agents should be observing. This is done by defining and configuring what Tap the agents should be listenining on. For this use case, the default PCAP (Packet Capture) tap configuration is all that is necessary for broad monitoring. Your agent tap configurations should look something like this: YAML JSON taps : default_pcap : interface : \"visor.module.input/1.0\" input_type : pcap config : iface : \"auto\" \"taps\" : { \"default_pcap\" : { \"config\" : { \"iface\" : \"auto\" , }, \"input_type\" : \"pcap\" , \"interface\" : \"visor.module.input/1.0\" } } If you want to explicitely specify the interface the PCAP tap should be listening on (instead of using the auto selection), please ensure that the iface is configured with the appropriate interface name. If the interface defined is not correct, you will need to re-run the agent provisioning command specifying the correct interface using the PKTVISOR_PCAP_IFACE_DEFAULT environment variable. Tip This monitoring configuration presumes that the Orb agents have full visibility on the network traffic of interest. This can be accomplished by mirroring the traffic to the configured interface or by configuring the interface to be in promiscuous mode. Configure Analysis The second step is to configure what analysis the agents should be doing on the data they are observing. This is done by defining and applying a Policy to the agents. Create a Policy The default policy created using the policy wizard is all that is necessary for broad monitoring. Please be sure to add the desired handlers to the policy, such as the NET , DNS and DHCP handlers. Your policy should look something like this: YAML JSON handlers : modules : handler_dns : type : dns handler_dhcp : type : dhcp handler_net : type : net input : input_type : pcap tap : default_pcap kind : collection { \"handlers\" : { \"modules\" : { \"handler_dns\" : { \"type\" : \"dns\" }, \"handler_dhcp\" : { \"type\" : \"dhcp\" }, \"handler_net\" : { \"type\" : \"net\" } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"default_pcap\" }, \"kind\" : \"collection\" } For a more tailored observability policy to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference . Apply your policy The final step is to apply your policy by creating a Dataset , which links your agents (through an Agent Group ) with your policy and the Sink where you want to send your metrics.","title":"Edge Networks"},{"location":"guides/edge_networks/#configure-observation","text":"The first step is to configure what data the agents should be observing. This is done by defining and configuring what Tap the agents should be listenining on. For this use case, the default PCAP (Packet Capture) tap configuration is all that is necessary for broad monitoring. Your agent tap configurations should look something like this: YAML JSON taps : default_pcap : interface : \"visor.module.input/1.0\" input_type : pcap config : iface : \"auto\" \"taps\" : { \"default_pcap\" : { \"config\" : { \"iface\" : \"auto\" , }, \"input_type\" : \"pcap\" , \"interface\" : \"visor.module.input/1.0\" } } If you want to explicitely specify the interface the PCAP tap should be listening on (instead of using the auto selection), please ensure that the iface is configured with the appropriate interface name. If the interface defined is not correct, you will need to re-run the agent provisioning command specifying the correct interface using the PKTVISOR_PCAP_IFACE_DEFAULT environment variable. Tip This monitoring configuration presumes that the Orb agents have full visibility on the network traffic of interest. This can be accomplished by mirroring the traffic to the configured interface or by configuring the interface to be in promiscuous mode.","title":"Configure Observation"},{"location":"guides/edge_networks/#configure-analysis","text":"The second step is to configure what analysis the agents should be doing on the data they are observing. This is done by defining and applying a Policy to the agents.","title":"Configure Analysis"},{"location":"guides/edge_networks/#create-a-policy","text":"The default policy created using the policy wizard is all that is necessary for broad monitoring. Please be sure to add the desired handlers to the policy, such as the NET , DNS and DHCP handlers. Your policy should look something like this: YAML JSON handlers : modules : handler_dns : type : dns handler_dhcp : type : dhcp handler_net : type : net input : input_type : pcap tap : default_pcap kind : collection { \"handlers\" : { \"modules\" : { \"handler_dns\" : { \"type\" : \"dns\" }, \"handler_dhcp\" : { \"type\" : \"dhcp\" }, \"handler_net\" : { \"type\" : \"net\" } } }, \"input\" : { \"input_type\" : \"pcap\" , \"tap\" : \"default_pcap\" }, \"kind\" : \"collection\" } For a more tailored observability policy to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference .","title":"Create a Policy"},{"location":"guides/edge_networks/#apply-your-policy","text":"The final step is to apply your policy by creating a Dataset , which links your agents (through an Agent Group ) with your policy and the Sink where you want to send your metrics.","title":"Apply your policy"},{"location":"guides/flow/","text":"This solution guide will walk you through setting up Observability for Network Traffic (Flow) using Orb agents. This is a monitoring configuration that can be used to collect and process network flow datagrams (sFlow, Netflow, IPFIX) at the edge to generate summarized metric streams. Configure Observation The first step is to configure the agents to accept inbound network flow datagrams. This is done by configuring the agents to enable the Flow input tap. Flow tap configuration The Flow tap supports sFlow, Netflow and IPFIX datagram formats. The flow_type attribute should be set to match your environment: flow_type: sflow to receive sFlow datagrams flow_type: netflow to receive either Netflow or IPFIX datagrams Your agent tap configuration should look something like this: sFlow Netflow/IPFIX visor : taps : my_flow_tap : input_type : flow config : port : 6343 bind : 192.168.1.1 flow_type : sflow tags : flow : true visor : taps : my_flow_tap : input_type : flow config : port : 9996 bind : 192.168.1.1 flow_type : netflow tags : flow : true Configure Analysis The second step is to configure the agents to analyze the received flow datagrams. This is done by defining and applying a tailored Policy to the agents, which specificies what filters to apply on the data (if any) and what metrics to generate. Create a basic Policy The following is a simple policy that allows you to observe from what sources you are receiving flow datagrams: YAML handlers : modules : flow_simple : type : flow metric_groups : enable : - by_bytes - top_interfaces disable : - all input : input_type : flow tap : my_flow_tap kind : collection Create a more advanced Policy The following is a more advanced policy that allows you to observe aggregated data based on ASNs, filtering on datagrams coming from specific devices and interfaces (expressed as SNMP index numbers): YAML handlers : modules : flow_advanced : config : summarize_ips_by_asn : true exclude_unknown_asns_from_summarization : true exclude_asns_from_summarization : - 16509 subnets_for_summarization : - 0.0.0.0/24 - ::/64 filter : only_device_interfaces : 10.10.10.1 : [ 1 , 2 , 3 ] 10.10.10.2 : [ 1 , 2 , 3 ] 10.10.10.3 : [ 1 , 2 , 3 ] type : flow metric_groups : enable : - counters - by_bytes - top_ports - top_ips - top_tos disable : - all input : input_type : flow tap : my_flow_tap kind : collection For more tailored Flow policies to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference . Applying your policy The final step is to apply your policy by creating a Dataset , which links your agents (through an Agent Group ) with your policy and the Sink where you want to send your metrics.","title":"Network Traffic (Flow)"},{"location":"guides/flow/#configure-observation","text":"The first step is to configure the agents to accept inbound network flow datagrams. This is done by configuring the agents to enable the Flow input tap.","title":"Configure Observation"},{"location":"guides/flow/#flow-tap-configuration","text":"The Flow tap supports sFlow, Netflow and IPFIX datagram formats. The flow_type attribute should be set to match your environment: flow_type: sflow to receive sFlow datagrams flow_type: netflow to receive either Netflow or IPFIX datagrams Your agent tap configuration should look something like this: sFlow Netflow/IPFIX visor : taps : my_flow_tap : input_type : flow config : port : 6343 bind : 192.168.1.1 flow_type : sflow tags : flow : true visor : taps : my_flow_tap : input_type : flow config : port : 9996 bind : 192.168.1.1 flow_type : netflow tags : flow : true","title":"Flow tap configuration"},{"location":"guides/flow/#configure-analysis","text":"The second step is to configure the agents to analyze the received flow datagrams. This is done by defining and applying a tailored Policy to the agents, which specificies what filters to apply on the data (if any) and what metrics to generate.","title":"Configure Analysis"},{"location":"guides/flow/#create-a-basic-policy","text":"The following is a simple policy that allows you to observe from what sources you are receiving flow datagrams: YAML handlers : modules : flow_simple : type : flow metric_groups : enable : - by_bytes - top_interfaces disable : - all input : input_type : flow tap : my_flow_tap kind : collection","title":"Create a basic Policy"},{"location":"guides/flow/#create-a-more-advanced-policy","text":"The following is a more advanced policy that allows you to observe aggregated data based on ASNs, filtering on datagrams coming from specific devices and interfaces (expressed as SNMP index numbers): YAML handlers : modules : flow_advanced : config : summarize_ips_by_asn : true exclude_unknown_asns_from_summarization : true exclude_asns_from_summarization : - 16509 subnets_for_summarization : - 0.0.0.0/24 - ::/64 filter : only_device_interfaces : 10.10.10.1 : [ 1 , 2 , 3 ] 10.10.10.2 : [ 1 , 2 , 3 ] 10.10.10.3 : [ 1 , 2 , 3 ] type : flow metric_groups : enable : - counters - by_bytes - top_ports - top_ips - top_tos disable : - all input : input_type : flow tap : my_flow_tap kind : collection For more tailored Flow policies to filter on specific traffic or to add (or exclude) specific metrics, please refer to the Orb Policy Reference .","title":"Create a more advanced Policy"},{"location":"guides/flow/#applying-your-policy","text":"The final step is to apply your policy by creating a Dataset , which links your agents (through an Agent Group ) with your policy and the Sink where you want to send your metrics.","title":"Applying your policy"},{"location":"guides/recursive_dns/","text":"This solution guide will walk you through setting up Observability for Recursive DNS Services using Orb agents. This guide is currently in draft. Please contact us to get an early preview.","title":"Recursive DNS"},{"location":"guides/response_testing/","text":"This solution guide will walk you through setting up Observability for Active Reponse Time Testing using Orb agents. This guide is currently in draft. Please contact us to get an early preview.","title":"Response Testing"}]}